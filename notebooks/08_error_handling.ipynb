{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Handling and Resilience\n",
    "\n",
    "This notebook demonstrates robust error handling, retry strategies, fallback mechanisms, and resilience patterns in lionpride. Learn how to build production-ready applications that gracefully handle failures.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Common errors** - Rate limits, timeouts, validation failures\n",
    "2. **Retry patterns** - Exponential backoff and intelligent retries\n",
    "3. **Circuit breakers** - Prevent cascading failures\n",
    "4. **Fallback chains** - Graceful degradation across providers\n",
    "5. **Validation recovery** - Handle and correct structured output errors\n",
    "6. **Timeout handling** - Set reasonable time boundaries\n",
    "7. **Production patterns** - Best practices for resilient systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "from lionpride import Session\n",
    "\n",
    "# Use lionpride's built-in concurrency primitives\n",
    "from lionpride.libs.concurrency import fail_after, retry\n",
    "from lionpride.operations import communicate\n",
    "from lionpride.services import iModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Error Handling\n",
    "\n",
    "Understanding common errors and how to handle them gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def basic_error_handling():\n",
    "    \"\"\"Handle common errors in LLM operations\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0)\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"error-demo\")\n",
    "\n",
    "    try:\n",
    "        result = await communicate(\n",
    "            session=session,\n",
    "            branch=branch,\n",
    "            parameters={\n",
    "                \"instruction\": \"Write a short poem about coding\",\n",
    "                \"imodel\": model.name,\n",
    "            },\n",
    "        )\n",
    "        print(f\"âœ“ Success: {result[:100]}...\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"âœ— Configuration error: {e}\")\n",
    "        # Invalid parameters, missing required fields\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âœ— Execution error: {e}\")\n",
    "        # API call failed, timeout, rate limit\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Unexpected error: {e}\")\n",
    "        # Catch-all for other errors\n",
    "\n",
    "\n",
    "# Run the example\n",
    "await basic_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Hierarchy\n",
    "\n",
    "```\n",
    "Exception\n",
    "â”œâ”€â”€ ValueError          # Configuration errors (invalid parameters)\n",
    "â”œâ”€â”€ RuntimeError        # Execution errors (API failures, timeouts)\n",
    "â”œâ”€â”€ ValidationError     # Pydantic validation failures\n",
    "â””â”€â”€ TimeoutError        # Operation timeouts\n",
    "```\n",
    "\n",
    "**Key principle**: Handle specific exceptions before general ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retry with Exponential Backoff\n",
    "\n",
    "Transient failures (network issues, temporary rate limits) can be resolved by retrying with increasing delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def communicate_with_retry(session, branch, parameters):\n",
    "    \"\"\"Retry operation with exponential backoff using lionpride's retry\"\"\"\n",
    "    return await retry(\n",
    "        lambda: communicate(session=session, branch=branch, parameters=parameters),\n",
    "        attempts=3,\n",
    "        base_delay=1.0,\n",
    "        max_delay=10.0,\n",
    "        retry_on=(RuntimeError,),  # Only retry runtime errors (API failures, timeouts)\n",
    "    )\n",
    "\n",
    "\n",
    "async def retry_pattern():\n",
    "    \"\"\"Use retry logic for transient failures\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0)\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"retry\")\n",
    "\n",
    "    try:\n",
    "        result = await communicate_with_retry(\n",
    "            session=session,\n",
    "            branch=branch,\n",
    "            parameters={\n",
    "                \"instruction\": \"Explain retry patterns in software engineering\",\n",
    "                \"imodel\": model.name,\n",
    "            },\n",
    "        )\n",
    "        print(f\"âœ“ Success after retries: {result[:150]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed after all retries: {e}\")\n",
    "\n",
    "\n",
    "# Run the example\n",
    "await retry_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Retry Configuration (lionpride.libs.concurrency.retry)\n\n- **attempts=3**: Maximum 3 attempts\n- **base_delay=1.0**: Start with 1 second delay\n- **max_delay=10.0**: Cap at 10 seconds\n- **retry_on=(RuntimeError,)**: Only retry RuntimeErrors (not config errors)\n- **jitter=0.1**: Small random variation to prevent thundering herd\n\n**Decision tree**:\n```\nError occurs\nâ”œâ”€â”€ Transient? (network, rate limit, timeout) â†’ Retry with backoff\nâ”œâ”€â”€ Configuration? (invalid params) â†’ Fix and retry\nâ”œâ”€â”€ Validation? (bad output) â†’ Retry with correction\nâ””â”€â”€ Permanent? (auth, quota) â†’ Fail fast\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Circuit Breaker Pattern\n",
    "\n",
    "Prevent cascading failures by \"opening\" the circuit after repeated failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker to prevent cascading failures\"\"\"\n",
    "\n",
    "    def __init__(self, failure_threshold=3, timeout=60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout  # seconds\n",
    "        self.failures = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = \"closed\"  # closed, open, half-open\n",
    "\n",
    "    def call(self, func):\n",
    "        \"\"\"Wrap function with circuit breaker logic\"\"\"\n",
    "\n",
    "        async def wrapper(*args, **kwargs):\n",
    "            if self.state == \"open\":\n",
    "                # Check if timeout has passed\n",
    "                if (datetime.now() - self.last_failure_time).seconds > self.timeout:\n",
    "                    print(\"âš ï¸  Circuit breaker: Transitioning to HALF-OPEN\")\n",
    "                    self.state = \"half-open\"\n",
    "                else:\n",
    "                    raise RuntimeError(\"Circuit breaker is OPEN - service unavailable\")\n",
    "\n",
    "            try:\n",
    "                result = await func(*args, **kwargs)\n",
    "\n",
    "                # Success - reset if half-open\n",
    "                if self.state == \"half-open\":\n",
    "                    print(\"âœ“ Circuit breaker: Transitioning to CLOSED\")\n",
    "                    self.state = \"closed\"\n",
    "                    self.failures = 0\n",
    "\n",
    "                return result\n",
    "\n",
    "            except Exception:\n",
    "                self.failures += 1\n",
    "                self.last_failure_time = datetime.now()\n",
    "\n",
    "                if self.failures >= self.failure_threshold:\n",
    "                    print(f\"âœ— Circuit breaker: OPENING after {self.failures} failures\")\n",
    "                    self.state = \"open\"\n",
    "\n",
    "                raise\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "async def circuit_breaker_demo():\n",
    "    \"\"\"Use circuit breaker to protect against repeated failures\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0)\n",
    "    session.services.register(model)\n",
    "\n",
    "    breaker = CircuitBreaker(failure_threshold=3, timeout=5)\n",
    "\n",
    "    @breaker.call\n",
    "    async def protected_communicate(instruction):\n",
    "        branch = session.create_branch(name=f\"attempt-{datetime.now().timestamp()}\")\n",
    "        return await communicate(\n",
    "            session=session,\n",
    "            branch=branch,\n",
    "            parameters={\n",
    "                \"instruction\": instruction,\n",
    "                \"imodel\": model.name,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Test with valid calls (should succeed)\n",
    "    print(\"Testing circuit breaker with valid calls:\\n\")\n",
    "\n",
    "    for i in range(2):\n",
    "        try:\n",
    "            result = await protected_communicate(f\"What is {i + 1} + {i + 1}?\")\n",
    "            print(f\"Attempt {i + 1}: âœ“ Success - {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {i + 1}: âœ— {e}\")\n",
    "        await asyncio.sleep(0.5)\n",
    "\n",
    "    print(f\"\\nCircuit breaker state: {breaker.state}\")\n",
    "    print(f\"Failures: {breaker.failures}\")\n",
    "\n",
    "\n",
    "# Run the example\n",
    "await circuit_breaker_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Breaker States\n",
    "\n",
    "```\n",
    "CLOSED (normal operation)\n",
    "    â†“ (failures >= threshold)\n",
    "OPEN (reject all calls)\n",
    "    â†“ (timeout expires)\n",
    "HALF-OPEN (test recovery)\n",
    "    â†“ (success)        â†“ (failure)\n",
    "  CLOSED             OPEN\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Prevents wasted requests to failing services\n",
    "- Allows service time to recover\n",
    "- Fails fast during outages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fallback Chain\n",
    "\n",
    "Graceful degradation by trying multiple providers until one succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fallback_chain():\n",
    "    \"\"\"Try multiple providers until one succeeds\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Register multiple models as fallbacks\n",
    "    providers = [\n",
    "        iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0, name=\"primary\"),\n",
    "        iModel(\n",
    "            provider=\"anthropic\",\n",
    "            endpoint=\"messages\",\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            temperature=0,\n",
    "            name=\"fallback1\",\n",
    "        ),\n",
    "        iModel(provider=\"gemini\", model=\"gemini-2.0-flash-exp\", temperature=0, name=\"fallback2\"),\n",
    "    ]\n",
    "\n",
    "    for model in providers:\n",
    "        session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"fallback\")\n",
    "    question = \"What is machine learning in one sentence?\"\n",
    "\n",
    "    # Try each provider in order\n",
    "    for i, model in enumerate(providers, 1):\n",
    "        try:\n",
    "            print(f\"Attempt {i}: Trying {model.name}...\")\n",
    "\n",
    "            result = await communicate(\n",
    "                session=session,\n",
    "                branch=branch,\n",
    "                parameters={\n",
    "                    \"instruction\": question,\n",
    "                    \"imodel\": model.name,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(f\"âœ“ Success with {model.name}\")\n",
    "            print(f\"Result: {result}\\n\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— {model.name} failed: {type(e).__name__}\")\n",
    "            if i == len(providers):\n",
    "                print(\"All providers failed!\")\n",
    "                raise\n",
    "            print(\"Falling back to next provider...\\n\")\n",
    "\n",
    "\n",
    "# Run the example\n",
    "await fallback_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallback Strategy\n",
    "\n",
    "**Order by**:\n",
    "1. **Primary**: Best quality/speed\n",
    "2. **Secondary**: Good balance\n",
    "3. **Tertiary**: Always available (or local model)\n",
    "\n",
    "**Benefits**:\n",
    "- High availability across provider outages\n",
    "- Cost optimization (try cheaper models first if appropriate)\n",
    "- Provider diversity reduces single points of failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation Error Recovery\n",
    "\n",
    "When structured outputs fail validation, retry with corrective instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractedData(BaseModel):\n",
    "    \"\"\"Structured data with validation\"\"\"\n",
    "\n",
    "    name: str = Field(..., min_length=1)\n",
    "    age: int = Field(..., ge=0, le=150)\n",
    "    email: str = Field(..., pattern=r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\")\n",
    "\n",
    "\n",
    "async def validate_and_recover():\n",
    "    \"\"\"Validate outputs and retry with corrections\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0)\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"validation\")\n",
    "\n",
    "    max_attempts = 3\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        print(f\"\\nAttempt {attempt}:\")\n",
    "\n",
    "        try:\n",
    "            result = await communicate(\n",
    "                session=session,\n",
    "                branch=branch,\n",
    "                parameters={\n",
    "                    \"instruction\": (\n",
    "                        \"Extract person info: 'John Doe, 32 years old, email: john.doe@example.com'\"\n",
    "                    ),\n",
    "                    \"imodel\": model.name,\n",
    "                    \"response_model\": ExtractedData,\n",
    "                    \"return_as\": \"model\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(\"âœ“ Validation passed!\")\n",
    "            print(f\"  Name: {result.name}\")\n",
    "            print(f\"  Age: {result.age}\")\n",
    "            print(f\"  Email: {result.email}\")\n",
    "            return result\n",
    "\n",
    "        except ValidationError as e:\n",
    "            print(f\"âœ— Validation failed: {e}\")\n",
    "\n",
    "            if attempt < max_attempts:\n",
    "                # Add corrective instruction\n",
    "                error_details = str(e)\n",
    "                print(\"  Retrying with correction...\")\n",
    "            else:\n",
    "                print(\"Max attempts reached!\")\n",
    "                raise\n",
    "\n",
    "\n",
    "# Run the example\n",
    "await validate_and_recover()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Recovery Pattern\n",
    "\n",
    "1. **Try extraction** with response_model\n",
    "2. **Catch ValidationError** with specific field failures\n",
    "3. **Send corrective instruction** with error details\n",
    "4. **Retry** with updated context\n",
    "5. **Fail after max attempts** to prevent infinite loops\n",
    "\n",
    "**Key**: Use Pydantic's detailed error messages to guide the model's correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Timeout Handling\n",
    "\n",
    "Set time boundaries to prevent indefinite waits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def timeout_handling():\n",
    "    \"\"\"Handle operation timeouts using lionpride's fail_after\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"timeout\")\n",
    "\n",
    "    try:\n",
    "        # Use lionpride's fail_after for deadline-aware timeout\n",
    "        with fail_after(30.0):  # 30 second timeout\n",
    "            result = await communicate(\n",
    "                session=session,\n",
    "                branch=branch,\n",
    "                parameters={\n",
    "                    \"instruction\": \"Write a brief summary about the history of computing\",\n",
    "                    \"imodel\": model.name,\n",
    "                    \"max_tokens\": 200,\n",
    "                },\n",
    "            )\n",
    "        print(f\"âœ“ Completed within timeout: {result}\")\n",
    "\n",
    "    except TimeoutError:\n",
    "        print(\"âœ— Operation timed out after 30 seconds\")\n",
    "\n",
    "        # Option: Retry with shorter response\n",
    "        print(\"Retrying with shorter max_tokens...\")\n",
    "        with fail_after(10.0):\n",
    "            result = await communicate(\n",
    "                session=session,\n",
    "                branch=branch,\n",
    "                parameters={\n",
    "                    \"instruction\": \"Write one sentence about computing history\",\n",
    "                    \"imodel\": model.name,\n",
    "                    \"max_tokens\": 50,\n",
    "                },\n",
    "            )\n",
    "        print(f\"âœ“ Completed: {result}\")\n",
    "\n",
    "\n",
    "# Run the example\n",
    "await timeout_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Timeout Primitives (lionpride.libs.concurrency)\n\n**fail_after(seconds)**: Raises TimeoutError if deadline exceeded\n**move_on_after(seconds)**: Silently cancels if deadline exceeded (no exception)\n\n**Recommended timeouts**:\n- **Short responses**: 5-10 seconds\n- **Medium responses**: 15-30 seconds\n- **Long responses**: 30-60 seconds\n- **Batch operations**: 60-300 seconds\n\n**Always set timeouts** to prevent hanging operations and resource leaks."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Rate Limit Handling\n",
    "\n",
    "Built-in rate limiting prevents hitting provider quotas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rate_limit_handling():\n",
    "    \"\"\"Handle rate limits gracefully\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Use built-in rate limiting\n",
    "    model = iModel(\n",
    "        provider=\"openai\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        limit_requests=10,  # Max 10 requests per minute\n",
    "        limit_tokens=10000,  # Max 10k tokens per minute\n",
    "        capacity_refresh_time=60,\n",
    "    )\n",
    "    session.services.register(model)\n",
    "\n",
    "    # Send multiple requests\n",
    "    num_requests = 5\n",
    "    print(f\"Sending {num_requests} requests with rate limit of 10/minute...\\n\")\n",
    "\n",
    "    results = []\n",
    "    for i in range(num_requests):\n",
    "        try:\n",
    "            branch = session.create_branch(name=f\"request-{i}\")\n",
    "            result = await communicate(\n",
    "                session=session,\n",
    "                branch=branch,\n",
    "                parameters={\n",
    "                    \"instruction\": f\"Question {i + 1}: What is {i + 1} squared?\",\n",
    "                    \"imodel\": model.name,\n",
    "                },\n",
    "            )\n",
    "            print(f\"Request {i + 1}: âœ“ {result}\")\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i + 1}: âœ— {e}\")\n",
    "\n",
    "    print(f\"\\nCompleted {len(results)}/{num_requests} requests\")\n",
    "\n",
    "\n",
    "# Run the example\n",
    "await rate_limit_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate Limit Configuration\n",
    "\n",
    "**Parameters**:\n",
    "- `limit_requests`: Max requests per time window\n",
    "- `limit_tokens`: Max tokens per time window\n",
    "- `capacity_refresh_time`: Time window in seconds\n",
    "\n",
    "**Automatic throttling** prevents 429 errors and quota violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Production-Ready Patterns\n\n### Best Practices\n\n#### 1. Fail Fast for Configuration Errors\n\n```python\n# Validate early\nif not session.services.has(model_name):\n    raise ValueError(f\"Model {model_name} not registered\")\n\n# Don't retry configuration errors\n```\n\n#### 2. Retry Transient Errors\n\n```python\nfrom lionpride.libs.concurrency import retry\n\n# Network errors, rate limits, timeouts\nresult = await retry(\n    lambda: communicate(session, branch, params),\n    attempts=3,\n    retry_on=(RuntimeError, ConnectionError, TimeoutError),\n)\n```\n\n#### 3. Log Failures\n\n```python\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    result = await communicate(...)\nexcept Exception as e:\n    logger.error(f\"Communication failed: {e}\", exc_info=True)\n    raise\n```\n\n#### 4. Always Use Timeouts\n\n```python\nfrom lionpride.libs.concurrency import fail_after\n\n# Always set reasonable timeouts\nwith fail_after(30.0):  # 30 seconds\n    result = await long_operation()\n```\n\n#### 5. Graceful Degradation\n\n```python\n# Provide fallback responses\ntry:\n    result = await ai_analysis()\nexcept Exception:\n    result = \"Analysis unavailable. Please try again later.\"\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls\n",
    "\n",
    "### 1. Catching Too Broad\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - masks real issues\n",
    "try:\n",
    "    result = await communicate(...)\n",
    "except:  # Catches everything!\n",
    "    pass\n",
    "\n",
    "# âœ… Right - specific exceptions\n",
    "try:\n",
    "    result = await communicate(...)\n",
    "except (ValueError, RuntimeError) as e:\n",
    "    handle_error(e)\n",
    "```\n",
    "\n",
    "### 2. Infinite Retries\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - no limit\n",
    "while True:\n",
    "    try:\n",
    "        return await communicate(...)\n",
    "    except:\n",
    "        continue  # Forever!\n",
    "\n",
    "# âœ… Right - max attempts\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        return await communicate(...)\n",
    "    except:\n",
    "        if attempt == max_retries - 1:\n",
    "            raise\n",
    "```\n",
    "\n",
    "### 3. Not Handling Async Errors\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - async error not awaited\n",
    "try:\n",
    "    task = communicate(...)  # Not awaited!\n",
    "except Exception:\n",
    "    pass  # Won't catch\n",
    "\n",
    "# âœ… Right - await first\n",
    "try:\n",
    "    result = await communicate(...)\n",
    "except Exception:\n",
    "    handle_error()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Handle specific exceptions** - Don't catch everything\n",
    "2. **Retry transient failures** - Use exponential backoff\n",
    "3. **Set timeouts** - Prevent hanging operations\n",
    "4. **Use circuit breakers** - Prevent cascading failures\n",
    "5. **Implement fallbacks** - Graceful degradation across providers\n",
    "6. **Validate outputs** - Retry with corrections\n",
    "7. **Rate limit proactively** - Prevent quota violations\n",
    "8. **Log failures** - Aid debugging and monitoring\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "- [ ] Specific exception handling\n",
    "- [ ] Retry logic with max attempts\n",
    "- [ ] Exponential backoff\n",
    "- [ ] Circuit breakers for critical paths\n",
    "- [ ] Fallback providers configured\n",
    "- [ ] Timeouts set on all async operations\n",
    "- [ ] Rate limits configured\n",
    "- [ ] Comprehensive logging\n",
    "- [ ] Graceful degradation\n",
    "- [ ] Monitoring and alerts\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Advanced patterns**: See `09_advanced_operations.ipynb`\n",
    "- **Testing**: See `10_testing.ipynb`\n",
    "- **Production deployment**: See documentation\n",
    "\n",
    "Build resilient, production-ready AI applications! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
