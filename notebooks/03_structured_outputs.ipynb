{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs with Pydantic\n",
    "\n",
    "Learn how to get validated, type-safe responses from LLMs using Pydantic models.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Using `response_model` for JSON Schema validation\n",
    "- How lionpride renders schemas to prompts\n",
    "- Handling validation failures and retries\n",
    "- Complex nested models\n",
    "- Combining structured outputs with reasoning\n",
    "\n",
    "**Prerequisites:** Basic understanding of Session, Branch, and communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from lionpride import Session\n",
    "from lionpride.libs.schema_handlers import typescript_schema\n",
    "from lionpride.operations import communicate, operate\n",
    "from lionpride.services import iModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Structured Output\n",
    "\n",
    "Pass a Pydantic model as `response_model` to get validated JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model for the expected output\n",
    "class Sentiment(BaseModel):\n",
    "    \"\"\"Sentiment analysis result.\"\"\"\n",
    "\n",
    "    text: str = Field(..., description=\"The analyzed text\")\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field(\n",
    "        ..., description=\"Overall sentiment\"\n",
    "    )\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence score (0-1)\")\n",
    "    key_phrases: list[str] = Field(\n",
    "        default_factory=list, description=\"Key phrases that influenced the sentiment\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Pydantic Model Schema:\")\n",
    "print(json.dumps(Sentiment.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_sentiment():\n",
    "    \"\"\"Get structured sentiment analysis.\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0)\n",
    "    session.services.register(model)\n",
    "    branch = session.create_branch(name=\"sentiment\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Analyze the sentiment of this review\",\n",
    "            \"context\": {\n",
    "                \"review\": \"The product exceeded my expectations! Fast shipping, \"\n",
    "                \"great quality, and the customer service was incredibly helpful.\"\n",
    "            },\n",
    "            \"imodel\": model.name,\n",
    "            \"response_model\": Sentiment,  # Request structured output\n",
    "            \"return_as\": \"model\",  # Return as Pydantic model instance\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Type: {type(result).__name__}\")\n",
    "    print(f\"Sentiment: {result.sentiment}\")\n",
    "    print(f\"Confidence: {result.confidence}\")\n",
    "    print(f\"Key phrases: {result.key_phrases}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = await analyze_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How lionpride Renders Schemas\n",
    "\n",
    "lionpride converts JSON Schema to compact TypeScript notation for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare JSON Schema vs TypeScript representation\n",
    "json_schema = Sentiment.model_json_schema()\n",
    "ts_schema = typescript_schema(json_schema)\n",
    "\n",
    "print(\"=== JSON Schema ===\")\n",
    "json_str = json.dumps(json_schema, indent=2)\n",
    "print(json_str)\n",
    "print(f\"\\nLength: {len(json_str)} chars, ~{len(json_str) // 4} tokens\")\n",
    "\n",
    "print(\"\\n=== TypeScript Schema ===\")\n",
    "print(ts_schema)\n",
    "print(f\"\\nLength: {len(ts_schema)} chars, ~{len(ts_schema) // 4} tokens\")\n",
    "\n",
    "reduction = (1 - len(ts_schema) / len(json_str)) * 100\n",
    "print(f\"\\nReduction: ~{reduction:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complex Nested Models\n",
    "\n",
    "Structured outputs work with arbitrarily complex nested Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nested models\n",
    "class Address(BaseModel):\n",
    "    street: str\n",
    "    city: str\n",
    "    country: str\n",
    "    postal_code: str\n",
    "\n",
    "\n",
    "class Employment(BaseModel):\n",
    "    company: str\n",
    "    position: str\n",
    "    start_year: int\n",
    "    end_year: int | None = None\n",
    "    is_current: bool = False\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Extracted person information.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"Full name\")\n",
    "    age: int | None = Field(None, description=\"Age if mentioned\")\n",
    "    occupation: str | None = Field(None, description=\"Current occupation\")\n",
    "    address: Address | None = Field(None, description=\"Address if mentioned\")\n",
    "    employment_history: list[Employment] = Field(default_factory=list, description=\"Work history\")\n",
    "    skills: list[str] = Field(default_factory=list, description=\"Skills/expertise\")\n",
    "\n",
    "\n",
    "# Show the TypeScript schema\n",
    "print(\"TypeScript Schema for Person:\")\n",
    "print(typescript_schema(Person.model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_person_info():\n",
    "    \"\"\"Extract structured person information from text.\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o\", temperature=0)\n",
    "    session.services.register(model)\n",
    "    branch = session.create_branch(name=\"extraction\")\n",
    "\n",
    "    bio = \"\"\"\n",
    "    Sarah Chen is a 32-year-old software architect based in San Francisco. \n",
    "    She currently works at TechCorp as VP of Engineering, a position she's \n",
    "    held since 2022. Before that, she was a senior engineer at StartupXYZ \n",
    "    from 2018 to 2021. Sarah is skilled in Python, distributed systems, \n",
    "    and machine learning. She lives at 123 Market St, San Francisco, CA 94102.\n",
    "    \"\"\"\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Extract structured information from this bio\",\n",
    "            \"context\": {\"bio\": bio},\n",
    "            \"imodel\": model.name,\n",
    "            \"response_model\": Person,\n",
    "            \"return_as\": \"model\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Name: {result.name}\")\n",
    "    print(f\"Age: {result.age}\")\n",
    "    print(f\"Occupation: {result.occupation}\")\n",
    "    print(\"\\nAddress:\")\n",
    "    if result.address:\n",
    "        print(f\"  {result.address.street}\")\n",
    "        print(f\"  {result.address.city}, {result.address.postal_code}\")\n",
    "        print(f\"  {result.address.country}\")\n",
    "    print(\"\\nEmployment History:\")\n",
    "    for job in result.employment_history:\n",
    "        current = \" (current)\" if job.is_current else \"\"\n",
    "        end = job.end_year or \"present\"\n",
    "        print(f\"  - {job.position} at {job.company} ({job.start_year}-{end}){current}\")\n",
    "    print(f\"\\nSkills: {', '.join(result.skills)}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = await extract_person_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation and Retries\n",
    "\n",
    "lionpride can automatically retry when the model produces invalid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrictScore(BaseModel):\n",
    "    \"\"\"Score with strict validation.\"\"\"\n",
    "\n",
    "    score: int = Field(..., ge=1, le=10, description=\"Score from 1 to 10\")\n",
    "    justification: str = Field(\n",
    "        ..., min_length=20, description=\"At least 20 character justification\"\n",
    "    )\n",
    "\n",
    "\n",
    "async def validated_scoring():\n",
    "    \"\"\"Get validated scores with retry on failure.\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0)\n",
    "    session.services.register(model)\n",
    "    branch = session.create_branch(name=\"scoring\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Rate this code quality from 1-10\",\n",
    "            \"context\": {\"code\": \"def add(a,b): return a+b\"},\n",
    "            \"imodel\": model.name,\n",
    "            \"response_model\": StrictScore,\n",
    "            \"return_as\": \"model\",\n",
    "            \"max_retries\": 2,  # Retry up to 2 times on validation failure\n",
    "            \"strict_validation\": False,  # Don't raise on failure\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if isinstance(result, dict) and result.get(\"validation_failed\"):\n",
    "        print(f\"Validation failed: {result['error']}\")\n",
    "        print(f\"Raw response: {result['raw'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"Score: {result.score}/10\")\n",
    "        print(f\"Justification: {result.justification}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = await validated_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using operate() with Reasoning\n",
    "\n",
    "The `operate()` function adds optional reasoning fields to structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision(BaseModel):\n",
    "    \"\"\"A decision with options analysis.\"\"\"\n",
    "\n",
    "    choice: str = Field(..., description=\"The recommended choice\")\n",
    "    alternatives: list[str] = Field(default_factory=list, description=\"Other options considered\")\n",
    "    risk_level: Literal[\"low\", \"medium\", \"high\"] = Field(..., description=\"Risk assessment\")\n",
    "\n",
    "\n",
    "async def decision_with_reasoning():\n",
    "    \"\"\"Get structured decision with explicit reasoning.\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o\", temperature=0)\n",
    "    session.services.register(model)\n",
    "    branch = session.create_branch(name=\"decision\")\n",
    "\n",
    "    result = await operate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Recommend a database for a high-traffic e-commerce site\",\n",
    "            \"context\": {\n",
    "                \"requirements\": [\n",
    "                    \"10M daily active users\",\n",
    "                    \"Complex product catalog\",\n",
    "                    \"Real-time inventory\",\n",
    "                    \"Strong consistency required\",\n",
    "                ]\n",
    "            },\n",
    "            \"imodel\": model.name,\n",
    "            \"response_model\": Decision,\n",
    "            \"reason\": True,  # Add reasoning field\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"=== Reasoning ===\")\n",
    "    if hasattr(result, \"reason\") and result.reason:\n",
    "        print(f\"Title: {result.reason.title}\")\n",
    "        print(f\"Confidence: {result.reason.confidence}\")\n",
    "        print(f\"Content: {result.reason.content[:200]}...\")\n",
    "\n",
    "    print(\"\\n=== Decision ===\")\n",
    "    if hasattr(result, \"decision\"):\n",
    "        print(f\"Choice: {result.decision.choice}\")\n",
    "        print(f\"Risk: {result.decision.risk_level}\")\n",
    "        print(f\"Alternatives: {result.decision.alternatives}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = await decision_with_reasoning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Enums and Literal Types\n",
    "\n",
    "Use `Literal` or `Enum` for constrained choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Priority(str, Enum):\n",
    "    CRITICAL = \"critical\"\n",
    "    HIGH = \"high\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LOW = \"low\"\n",
    "\n",
    "\n",
    "class TaskClassification(BaseModel):\n",
    "    \"\"\"Task classification result.\"\"\"\n",
    "\n",
    "    task: str = Field(..., description=\"The task description\")\n",
    "    category: Literal[\"bug\", \"feature\", \"refactor\", \"docs\", \"test\"] = Field(\n",
    "        ..., description=\"Task category\"\n",
    "    )\n",
    "    priority: Priority = Field(..., description=\"Priority level\")\n",
    "    estimated_hours: float = Field(..., ge=0.5, le=40, description=\"Estimated hours\")\n",
    "    tags: list[str] = Field(default_factory=list, description=\"Relevant tags\")\n",
    "\n",
    "\n",
    "async def classify_task():\n",
    "    \"\"\"Classify a task with constrained options.\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0)\n",
    "    session.services.register(model)\n",
    "    branch = session.create_branch(name=\"classify\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Classify this task\",\n",
    "            \"context\": {\n",
    "                \"task_description\": \"The login page crashes when users enter \"\n",
    "                \"special characters in the password field. \"\n",
    "                \"This affects production users.\"\n",
    "            },\n",
    "            \"imodel\": model.name,\n",
    "            \"response_model\": TaskClassification,\n",
    "            \"return_as\": \"model\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Task: {result.task}\")\n",
    "    print(f\"Category: {result.category}\")\n",
    "    print(f\"Priority: {result.priority.value}\")\n",
    "    print(f\"Estimated: {result.estimated_hours} hours\")\n",
    "    print(f\"Tags: {result.tags}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = await classify_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lists and Multiple Items\n",
    "\n",
    "Extract multiple structured items from content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(BaseModel):\n",
    "    \"\"\"Named entity.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    entity_type: Literal[\"person\", \"organization\", \"location\", \"date\", \"money\"]\n",
    "    context: str = Field(..., description=\"How the entity appears in text\")\n",
    "\n",
    "\n",
    "class EntityExtractionResult(BaseModel):\n",
    "    \"\"\"Named entity extraction result.\"\"\"\n",
    "\n",
    "    entities: list[Entity] = Field(default_factory=list)\n",
    "    total_count: int = Field(..., description=\"Total entities found\")\n",
    "\n",
    "\n",
    "async def extract_entities():\n",
    "    \"\"\"Extract multiple entities from text.\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0)\n",
    "    session.services.register(model)\n",
    "    branch = session.create_branch(name=\"ner\")\n",
    "\n",
    "    text = \"\"\"\n",
    "    Apple Inc. announced yesterday that CEO Tim Cook will visit \n",
    "    their new $5 billion campus in Austin, Texas on March 15, 2025.\n",
    "    The meeting with Governor Abbott is expected to discuss the \n",
    "    company's $1 billion investment in local manufacturing.\n",
    "    \"\"\"\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Extract all named entities from this text\",\n",
    "            \"context\": {\"text\": text},\n",
    "            \"imodel\": model.name,\n",
    "            \"response_model\": EntityExtractionResult,\n",
    "            \"return_as\": \"model\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Found {result.total_count} entities:\\n\")\n",
    "    for entity in result.entities:\n",
    "        print(f\"  [{entity.entity_type}] {entity.name}\")\n",
    "        print(f\"    Context: {entity.context}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = await extract_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `response_model` | Pydantic model for JSON Schema validation |\n",
    "| `return_as=\"model\"` | Return validated Pydantic instance |\n",
    "| `max_retries` | Retry count on validation failure |\n",
    "| `strict_validation` | Raise exception on failure (default: False) |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use `Field(description=...)` for better LLM understanding\n",
    "- Use `Literal` for constrained string choices\n",
    "- Use validators (`ge`, `le`, `min_length`) for data quality\n",
    "- Add `default_factory=list` for optional lists\n",
    "- Use `reason=True` with `operate()` for chain-of-thought\n",
    "\n",
    "### Schema Efficiency\n",
    "\n",
    "lionpride uses TypeScript notation in prompts, typically achieving **60-80% token reduction** compared to raw JSON Schema. This happens automatically when you use `response_model`.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- See notebook 03_lndl.ipynb for LNDL cognitive programming\n",
    "- See notebook 04_tool_calling.ipynb for ReAct with tools\n",
    "- See notebook 05_multi_turn.ipynb for conversation context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
