{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Your First Chat with lionpride\n",
    "\n",
    "**Goal**: Learn the basics of lionpride by building a simple chatbot in 5 minutes.\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "- Create a `Session` and `iModel` to interact with LLMs\n",
    "- Send a single message and get a response\n",
    "- Have a multi-turn conversation\n",
    "- Access conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install lionpride\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionpride import Session\n",
    "from lionpride.operations.operate import communicate\n",
    "from lionpride.services import iModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Session and Model\n",
    "\n",
    "- **Session**: Container for conversations, services, and state\n",
    "- **iModel**: Unified interface for any LLM provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session\n",
    "session = Session()\n",
    "\n",
    "# Create model (OpenAI)\n",
    "model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Register model with session\n",
    "session.services.register(model)\n",
    "\n",
    "print(f\"âœ“ Session created: {session.id}\")\n",
    "print(f\"âœ“ Model registered: {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Branch\n",
    "\n",
    "A **Branch** is a conversation thread (message history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = session.create_branch(name=\"main\")\n",
    "print(f\"âœ“ Branch created: {branch.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Send Your First Message\n",
    "\n",
    "Use `communicate()` for stateful chat - it persists messages to the branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await communicate(\n",
    "    session=session,\n",
    "    branch=branch,\n",
    "    parameters={\n",
    "        \"instruction\": \"Write a haiku about Python programming\",\n",
    "        \"imodel\": model.name,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"ðŸ¤– Response:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Multi-Turn Conversation\n",
    "\n",
    "The branch automatically maintains conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await communicate(\n",
    "    session=session,\n",
    "    branch=branch,\n",
    "    parameters={\n",
    "        \"instruction\": \"Now write one about async Python\",\n",
    "        \"imodel\": model.name,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"ðŸ¤– Response:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Access Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ“œ Conversation History ({len(branch)} messages):\\n\")\n",
    "\n",
    "for i, msg_id in enumerate(branch.order, 1):\n",
    "    message = session.messages[msg_id]\n",
    "    role = message.role.value\n",
    "\n",
    "    # Get content preview\n",
    "    if hasattr(message.content, \"instruction\"):\n",
    "        preview = message.content.instruction[:80]\n",
    "    elif hasattr(message.content, \"assistant_response\"):\n",
    "        preview = message.content.assistant_response[:80]\n",
    "    else:\n",
    "        preview = str(message.content)[:80]\n",
    "\n",
    "    print(f\"{i}. [{role}]: {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching Providers\n",
    "\n",
    "Same API works with any provider - just change the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic Claude\n",
    "# claude = iModel(provider=\"anthropic\", endpoint=\"messages\", model=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "# Google Gemini\n",
    "# gemini = iModel(provider=\"gemini\", model=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "# Groq (fast inference)\n",
    "# groq = iModel(provider=\"groq\", model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "print(\"âœ“ Same communicate() API works with all providers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- âœ… Create Session and register iModel\n",
    "- âœ… Create conversation branch\n",
    "- âœ… Send messages with `communicate()`\n",
    "- âœ… Multi-turn conversations with automatic history\n",
    "- âœ… Switch providers (OpenAI, Anthropic, Gemini, Groq)\n",
    "\n",
    "**Next**: Check out `02_chat_providers.ipynb` for more provider examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
