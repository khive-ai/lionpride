{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming: Real-Time Token Output\n",
    "\n",
    "Learn how to stream LLM responses token-by-token for better user experience and real-time feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Streaming Matters\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Traditional LLM calls wait for the entire response before displaying anything:\n",
    "\n",
    "```python\n",
    "# Non-streaming: Wait 10+ seconds, then see everything at once\n",
    "response = await model.invoke(...)  # üò¥ User waits...\n",
    "print(response)  # üí• Everything appears at once\n",
    "```\n",
    "\n",
    "### The Solution\n",
    "\n",
    "Streaming provides tokens as they're generated:\n",
    "\n",
    "```python\n",
    "# Streaming: See tokens immediately as they arrive\n",
    "async for chunk in model.invoke(...):\n",
    "    print(chunk.data, end=\"\", flush=True)  # ‚ö° Real-time output\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **Better UX**: User sees immediate feedback\n",
    "2. **Perceived Speed**: Feels faster even if total time is same\n",
    "3. **Memory Efficiency**: Process chunks without buffering entire response\n",
    "4. **Progressive Rendering**: Update UI as data arrives\n",
    "5. **Early Termination**: Stop generation if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "from lionpride import Session\n",
    "from lionpride.services import iModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Streaming with generate()\n",
    "\n",
    "The simplest way to stream: enable `stream=True` and iterate over chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def basic_streaming():\n",
    "    \"\"\"Stream tokens as they're generated\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Create model (streaming is enabled via invoke_stream method)\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    # Stream response using invoke_stream()\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    response_chunks = []\n",
    "    async for chunk in model.invoke_stream(\n",
    "        model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Write a haiku about coding\"}]\n",
    "    ):\n",
    "        response_chunks.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)  # Print immediately\n",
    "\n",
    "    print()  # Newline at end\n",
    "\n",
    "    full_response = \"\".join(response_chunks)\n",
    "    print(f\"\\n‚úÖ Received {len(response_chunks)} chunks ({len(full_response)} chars)\")\n",
    "\n",
    "    return full_response\n",
    "\n",
    "\n",
    "# Run the example\n",
    "result = await basic_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing Stream vs Non-Stream\n",
    "\n",
    "Let's see the timing difference side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compare_streaming():\n",
    "    \"\"\"Compare streaming vs non-streaming\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Create model\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    instruction = \"Explain async/await in Python in 2 paragraphs\"\n",
    "    messages = [{\"role\": \"user\", \"content\": instruction}]\n",
    "\n",
    "    # Non-streaming: Wait for everything\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NON-STREAMING (wait for complete response)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    start = time.time()\n",
    "    calling = await model.invoke(model=\"gpt-4o-mini\", messages=messages)\n",
    "    elapsed_sync = time.time() - start\n",
    "\n",
    "    # Extract response from calling object\n",
    "    response_sync = calling.execution.response.data if calling.execution.response else \"\"\n",
    "\n",
    "    print(f\"‚è±Ô∏è  Waited {elapsed_sync:.2f}s, then got everything at once:\\n\")\n",
    "    print(str(response_sync)[:200] + \"...\\n\")\n",
    "\n",
    "    # Streaming: See tokens immediately\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STREAMING (tokens appear in real-time)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    start = time.time()\n",
    "    first_token_time = None\n",
    "    chunks = []\n",
    "\n",
    "    async for chunk in model.invoke_stream(model=\"gpt-4o-mini\", messages=messages):\n",
    "        if first_token_time is None:\n",
    "            first_token_time = time.time() - start\n",
    "        chunks.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    elapsed_stream = time.time() - start\n",
    "\n",
    "    print(f\"\\n\\n‚ö° First token after {first_token_time:.2f}s\")\n",
    "    print(f\"‚è±Ô∏è  Total time: {elapsed_stream:.2f}s\")\n",
    "    print(f\"\\n‚ú® User perceived {(elapsed_sync / first_token_time):.1f}x faster!\")\n",
    "\n",
    "\n",
    "await compare_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming with Progress Indicators\n",
    "\n",
    "Show progress while streaming for long responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_with_progress():\n",
    "    \"\"\"Show progress while streaming\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how transformers work in machine learning (detailed explanation)\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"Generating response\", end=\"\", flush=True)\n",
    "\n",
    "    tokens_received = 0\n",
    "    response_chunks = []\n",
    "    words_seen = 0\n",
    "\n",
    "    async for chunk in model.invoke_stream(model=\"gpt-4o-mini\", messages=messages):\n",
    "        tokens_received += 1\n",
    "        response_chunks.append(chunk)\n",
    "\n",
    "        # Approximate word count\n",
    "        words_seen += len(chunk.split())\n",
    "\n",
    "        # Show progress every 10 tokens\n",
    "        if tokens_received % 10 == 0:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    print(f\"\\n\\n‚úÖ Received {tokens_received} chunks (~{words_seen} words)\\n\")\n",
    "    print(\"Response:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\".join(response_chunks))\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return \"\".join(response_chunks)\n",
    "\n",
    "\n",
    "result = await stream_with_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming with Conversation Context\n",
    "\n",
    "Stream responses while maintaining conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_with_context():\n",
    "    \"\"\"Stream responses while maintaining conversation context\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(\n",
    "        provider=\"anthropic\",\n",
    "        endpoint=\"messages\",\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"streaming-chat\")\n",
    "\n",
    "    # Turn 1: Build context using communicate()\n",
    "    from lionpride.operations import communicate\n",
    "\n",
    "    print(\"User: Tell me about Python decorators\\n\")\n",
    "\n",
    "    first_response = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Tell me about Python decorators in 2 sentences\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Assistant: {first_response}\\n\")\n",
    "\n",
    "    # Turn 2: Streaming follow-up with context\n",
    "    print(\"User: Can you show me a practical example?\\n\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    # Get conversation history for streaming\n",
    "    from lionpride.session.messages.utils import prepare_messages_for_chat\n",
    "\n",
    "    messages = [session.messages[msg_id] for msg_id in branch.order]\n",
    "    chat_messages = list(prepare_messages_for_chat(messages=messages, progression=branch))\n",
    "\n",
    "    # Add new instruction\n",
    "    chat_messages.append({\"role\": \"user\", \"content\": \"Can you show me a practical example?\"})\n",
    "\n",
    "    # Stream response\n",
    "    full_response = []\n",
    "    async for chunk in model.invoke_stream(\n",
    "        model=\"claude-3-5-sonnet-20241022\", messages=chat_messages\n",
    "    ):\n",
    "        full_response.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Save streamed response to branch\n",
    "    from lionpride.session.messages import AssistantResponseContent, Message\n",
    "\n",
    "    response_message = Message(\n",
    "        content=AssistantResponseContent(assistant_response=\"\".join(full_response))\n",
    "    )\n",
    "    session.add_message(response_message, branches=branch)\n",
    "\n",
    "    print(f\"‚úÖ Conversation has {len(branch.order)} messages\")\n",
    "\n",
    "    return session, branch\n",
    "\n",
    "\n",
    "session, branch = await stream_with_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a Streaming Chat UI Pattern\n",
    "\n",
    "A reusable pattern for chat applications with streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingChat:\n",
    "    \"\"\"Reusable streaming chat pattern\"\"\"\n",
    "\n",
    "    def __init__(self, model: iModel):\n",
    "        self.model = model\n",
    "        self.session = Session()\n",
    "        self.session.services.register(model)\n",
    "        self.branch = self.session.create_branch(name=\"main\")\n",
    "\n",
    "    async def chat(self, user_input: str, show_stats: bool = True):\n",
    "        \"\"\"Send message and stream response\"\"\"\n",
    "        from lionpride.session.messages import AssistantResponseContent, InstructionContent, Message\n",
    "        from lionpride.session.messages.utils import prepare_messages_for_chat\n",
    "\n",
    "        # Display user message\n",
    "        print(f\"\\nüë§ User: {user_input}\")\n",
    "        print(\"ü§ñ Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "        # Get conversation history\n",
    "        messages = [self.session.messages[msg_id] for msg_id in self.branch.order]\n",
    "        chat_messages = list(prepare_messages_for_chat(messages=messages, progression=self.branch))\n",
    "\n",
    "        # Add new message\n",
    "        chat_messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Stream response with stats\n",
    "        start_time = time.time()\n",
    "        first_token_time = None\n",
    "        chunks = []\n",
    "\n",
    "        async for chunk in self.model.invoke_stream(\n",
    "            model=self.model.backend.config.model, messages=chat_messages\n",
    "        ):\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time() - start_time\n",
    "\n",
    "            chunks.append(chunk)\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        full_response = \"\".join(chunks)\n",
    "\n",
    "        print()  # Newline\n",
    "\n",
    "        if show_stats:\n",
    "            print(\n",
    "                f\"\\nüìä Stats: {len(chunks)} chunks | \"\n",
    "                f\"First token: {first_token_time:.2f}s | \"\n",
    "                f\"Total: {total_time:.2f}s\"\n",
    "            )\n",
    "\n",
    "        # Save to conversation\n",
    "        user_msg = Message(content=InstructionContent(instruction=user_input))\n",
    "        response_msg = Message(content=AssistantResponseContent(assistant_response=full_response))\n",
    "\n",
    "        self.session.add_message(user_msg, branches=self.branch)\n",
    "        self.session.add_message(response_msg, branches=self.branch)\n",
    "\n",
    "        return full_response\n",
    "\n",
    "    def history(self):\n",
    "        \"\"\"Display conversation history\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"CONVERSATION HISTORY\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for msg_id in self.branch.order:\n",
    "            msg = self.session.messages[msg_id]\n",
    "            role = \"üë§\" if msg.role.value == \"user\" else \"ü§ñ\"\n",
    "\n",
    "            if hasattr(msg.content, \"instruction\"):\n",
    "                content = msg.content.instruction\n",
    "            elif hasattr(msg.content, \"assistant_response\"):\n",
    "                content = msg.content.assistant_response\n",
    "            else:\n",
    "                content = str(msg.content)\n",
    "\n",
    "            preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"{role} {msg.role.value}: {preview}\\n\")\n",
    "\n",
    "\n",
    "# Create streaming chat instance\n",
    "model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "chat = StreamingChat(model)\n",
    "\n",
    "# Have a multi-turn conversation\n",
    "await chat.chat(\"What are the key benefits of async programming?\")\n",
    "await chat.chat(\"Can you give me a code example?\")\n",
    "await chat.chat(\"What are common mistakes to avoid?\")\n",
    "\n",
    "# Show history\n",
    "chat.history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parallel Streaming from Multiple Models\n",
    "\n",
    "Compare responses from different models streaming simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_one_model(model: iModel, instruction: str, label: str):\n",
    "    \"\"\"Stream from one model with label\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": instruction}]\n",
    "\n",
    "    print(f\"\\n[{label}] Starting...\")\n",
    "    response = []\n",
    "\n",
    "    start = time.time()\n",
    "    async for chunk in model.invoke_stream(model=model.backend.config.model, messages=messages):\n",
    "        response.append(chunk)\n",
    "        # Show first few chunks\n",
    "        if len(response) <= 5:\n",
    "            print(f\"[{label}] {chunk}\", end=\"\", flush=True)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    full_response = \"\".join(response)\n",
    "    print(f\"\\n[{label}] Complete ({len(response)} chunks, {elapsed:.2f}s)\")\n",
    "\n",
    "    return full_response\n",
    "\n",
    "\n",
    "async def parallel_streaming():\n",
    "    \"\"\"Stream from multiple models simultaneously\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    gpt = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7, name=\"gpt\")\n",
    "\n",
    "    claude = iModel(\n",
    "        provider=\"anthropic\",\n",
    "        endpoint=\"messages\",\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        temperature=0.7,\n",
    "        name=\"claude\",\n",
    "    )\n",
    "\n",
    "    session.services.register(gpt)\n",
    "    session.services.register(claude)\n",
    "\n",
    "    question = \"What makes a good API design? Answer in 3 bullet points.\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Streaming from both models: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Stream from both models in parallel\n",
    "    results = await asyncio.gather(\n",
    "        stream_one_model(gpt, question, \"GPT\"),\n",
    "        stream_one_model(claude, question, \"Claude\"),\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RESPONSES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nü§ñ GPT Response ({len(results[0])} chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(results[0])\n",
    "\n",
    "    print(f\"\\nü§ñ Claude Response ({len(results[1])} chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(results[1])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = await parallel_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Buffered Streaming\n",
    "\n",
    "Buffer chunks before displaying for smoother output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def buffered_stream(model: iModel, messages: list, buffer_size: int = 5):\n",
    "    \"\"\"Buffer chunks before displaying\"\"\"\n",
    "    print(f\"Buffering every {buffer_size} chunks...\\n\")\n",
    "    print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "    buffer = []\n",
    "    total_chunks = 0\n",
    "\n",
    "    async for chunk in model.invoke_stream(model=model.backend.config.model, messages=messages):\n",
    "        buffer.append(chunk)\n",
    "        total_chunks += 1\n",
    "\n",
    "        if len(buffer) >= buffer_size:\n",
    "            # Flush buffer\n",
    "            print(\"\".join(buffer), end=\"\", flush=True)\n",
    "            buffer = []\n",
    "\n",
    "    # Flush remaining\n",
    "    if buffer:\n",
    "        print(\"\".join(buffer), end=\"\", flush=True)\n",
    "\n",
    "    print(f\"\\n\\n‚úÖ Buffered {total_chunks} chunks (flushed every {buffer_size} chunks)\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "session = Session()\n",
    "model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "session.services.register(model)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Explain the benefits of microservices architecture\"}]\n",
    "\n",
    "await buffered_stream(model, messages, buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling in Streams\n",
    "\n",
    "Handle errors gracefully during streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def safe_stream(model: iModel, messages: list):\n",
    "    \"\"\"Stream with error handling\"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    try:\n",
    "        print(\"Streaming with error handling...\\n\")\n",
    "        print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "        async for chunk in model.invoke_stream(model=model.backend.config.model, messages=messages):\n",
    "            chunks.append(chunk)\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "\n",
    "        print(\"\\n\\n‚úÖ Streaming completed successfully\")\n",
    "        print(f\"Received {len(chunks)} chunks\")\n",
    "\n",
    "    except TimeoutError:\n",
    "        print(f\"\\n\\n‚è±Ô∏è  Streaming timeout after {len(chunks)} chunks\")\n",
    "        print(\"Partial response available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Streaming error: {type(e).__name__}: {e}\")\n",
    "        print(f\"Received {len(chunks)} chunks before error\")\n",
    "\n",
    "    finally:\n",
    "        # Always return partial response\n",
    "        partial_response = \"\".join(chunks)\n",
    "        print(f\"\\nPartial response length: {len(partial_response)} chars\")\n",
    "        return partial_response\n",
    "\n",
    "\n",
    "# Example usage\n",
    "session = Session()\n",
    "model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "session.services.register(model)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Explain the concept of event-driven architecture\"}]\n",
    "\n",
    "result = await safe_stream(model, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Concepts\n\n### Streaming Architecture\n\n```\nLLM Provider ‚Üí HTTP SSE/Streaming ‚Üí lionpride iModel ‚Üí invoke_stream() ‚Üí async iterator ‚Üí Your code\n```\n\n### Stream vs Non-Stream\n\n```python\n# Non-streaming: Wait for complete response\nmodel = iModel(provider=\"openai\", model=\"gpt-4o-mini\")\ncalling = await model.invoke(...)  # Returns Calling object\nresponse = calling.execution.response.data\n\n# Streaming: Iterate over chunks\nasync for chunk in model.invoke_stream(...):  # Yields string chunks\n    process(chunk)\n```\n\n### Chunk Structure\n\n```python\n# Each chunk is a string (text content)\nasync for chunk in model.invoke_stream(...):\n    print(chunk)  # Direct string output\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Common Pitfalls\n\n### 1. Using invoke() instead of invoke_stream()\n\n```python\n# ‚ùå Wrong - invoke() returns Calling, not iterator\nasync for chunk in model.invoke(...):  # TypeError!\n\n# ‚úÖ Right - use invoke_stream() for streaming\nasync for chunk in model.invoke_stream(...):  # Works\n```\n\n### 2. Not flushing print buffer\n\n```python\n# ‚ùå Wrong - output may be buffered\nprint(chunk, end=\"\")\n\n# ‚úÖ Right - flush immediately\nprint(chunk, end=\"\", flush=True)\n```\n\n### 3. Blocking operations in stream loop\n\n```python\n# ‚ùå Wrong - blocks streaming\nasync for chunk in model.invoke_stream(...):\n    time.sleep(0.1)  # Don't block!\n    print(chunk)\n\n# ‚úÖ Right - use async sleep if needed\nasync for chunk in model.invoke_stream(...):\n    await asyncio.sleep(0.1)  # Non-blocking\n    print(chunk)\n```\n\n### 4. Not handling incomplete chunks\n\n```python\n# ‚ùå Wrong - assumes complete words\nasync for chunk in model.invoke_stream(...):\n    words = chunk.split()  # May split mid-word!\n\n# ‚úÖ Right - accumulate and process complete units\nbuffer = \"\"\nasync for chunk in model.invoke_stream(...):\n    buffer += chunk\n    # Process complete sentences/paragraphs\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Streaming\n",
    "\n",
    "### ‚úÖ Use streaming for:\n",
    "\n",
    "- Long responses (>100 tokens)\n",
    "- Interactive applications\n",
    "- Real-time feedback requirements\n",
    "- Large-scale generation\n",
    "- Better perceived performance\n",
    "\n",
    "### ‚ùå Skip streaming for:\n",
    "\n",
    "- Short responses (<50 tokens)\n",
    "- Batch processing\n",
    "- Responses needing validation before display\n",
    "- Structured output (wait for complete JSON)\n",
    "- When latency isn't critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've learned how to:\n\n1. ‚úÖ **Stream responses** with `model.invoke_stream()`\n2. ‚úÖ **Iterate over chunks** with `async for`\n3. ‚úÖ **Display real-time output** with `flush=True`\n4. ‚úÖ **Track progress** with chunk counters\n5. ‚úÖ **Maintain context** across turns\n6. ‚úÖ **Build chat patterns** with streaming\n7. ‚úÖ **Stream in parallel** from multiple models\n8. ‚úÖ **Handle errors** gracefully\n\n### Key Takeaways\n\n- **Use `invoke_stream()`** for streaming (not `invoke()`)\n- **First token latency** matters more than total time\n- **Always flush output** for real-time display\n- **Buffer wisely** for smoother output\n- **Handle errors** to preserve partial responses\n\n### Next Steps\n\n- **Notebook 08**: Error handling patterns\n- **Cookbook**: [Error Handling](../docs/cookbook/error_handling.md)\n\n**Streaming makes LLM applications feel faster and more responsive. Use it for better user experience!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
