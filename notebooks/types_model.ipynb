{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashableModel - Content-Based Hashable Pydantic Model\n",
    "\n",
    "HashableModel provides content-based hashing for value equality, unlike Element's ID-based hashing. Use when identical field values should be treated as duplicates.\n",
    "\n",
    "**Core Features:**\n",
    "- **Content-Based Equality**: Identical fields → same hash (vs Element's ID-based)\n",
    "- **Immutable by Default**: Frozen to prevent hash corruption in sets/dicts\n",
    "- **Deduplication**: Safe for `to_list(unique=True)` and set operations\n",
    "- **Cache Keys**: Identical configs produce same hash for caching\n",
    "- **Multi-Mode Serialization**: `python`/`json` modes for different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:27.773937Z",
     "iopub.status.busy": "2025-11-23T18:54:27.773629Z",
     "iopub.status.idle": "2025-11-23T18:54:28.067482Z",
     "shell.execute_reply": "2025-11-23T18:54:28.038837Z"
    }
   },
   "outputs": [],
   "source": [
    "from lionpride.core import Element\n",
    "from lionpride.ln import to_list\n",
    "from lionpride.types.model import HashableModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Construction\n",
    "\n",
    "Define custom models by subclassing HashableModel. All fields contribute to the hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.142262Z",
     "iopub.status.busy": "2025-11-23T18:54:28.142103Z",
     "iopub.status.idle": "2025-11-23T18:54:28.184885Z",
     "shell.execute_reply": "2025-11-23T18:54:28.178019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config1: name='gpt-4' temperature=0.7 max_tokens=1000\n",
      "config2: name='gpt-4' temperature=0.7 max_tokens=1000\n",
      "config3: name='gpt-4' temperature=0.9 max_tokens=1000\n"
     ]
    }
   ],
   "source": [
    "class Config(HashableModel):\n",
    "    \"\"\"Example configuration object.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 1000\n",
    "\n",
    "\n",
    "# Create instances\n",
    "config1 = Config(name=\"gpt-4\", temperature=0.7)\n",
    "config2 = Config(name=\"gpt-4\", temperature=0.7)  # Identical values\n",
    "config3 = Config(name=\"gpt-4\", temperature=0.9)  # Different temperature\n",
    "\n",
    "print(f\"config1: {config1}\")\n",
    "print(f\"config2: {config2}\")\n",
    "print(f\"config3: {config3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Content-Based Equality vs Element's ID-Based\n",
    "\n",
    "**HashableModel**: Equality based on field values (content)  \n",
    "**Element**: Equality based on ID (identity)\n",
    "\n",
    "This is the fundamental difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.366074Z",
     "iopub.status.busy": "2025-11-23T18:54:28.365757Z",
     "iopub.status.idle": "2025-11-23T18:54:28.372991Z",
     "shell.execute_reply": "2025-11-23T18:54:28.372002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HashableModel (Content-Based) ===\n",
      "config1 == config2 (same values): True\n",
      "config1 == config3 (different values): False\n",
      "hash(config1) == hash(config2): True\n",
      "hash(config1) == hash(config3): False\n",
      "\n",
      "=== Element (ID-Based) ===\n",
      "elem1 == elem2 (same ID, different metadata): True\n",
      "elem1 == elem3 (different ID, same metadata): False\n",
      "hash(elem1) == hash(elem2): True\n",
      "hash(elem1) == hash(elem3): False\n"
     ]
    }
   ],
   "source": [
    "# HashableModel - content equality\n",
    "print(\"=== HashableModel (Content-Based) ===\")\n",
    "print(f\"config1 == config2 (same values): {config1 == config2}\")\n",
    "print(f\"config1 == config3 (different values): {config1 == config3}\")\n",
    "print(f\"hash(config1) == hash(config2): {hash(config1) == hash(config2)}\")\n",
    "print(f\"hash(config1) == hash(config3): {hash(config1) == hash(config3)}\")\n",
    "\n",
    "print(\"\\n=== Element (ID-Based) ===\")\n",
    "elem1 = Element(metadata={\"value\": 1})\n",
    "elem2 = Element(id=elem1.id, metadata={\"value\": 999})  # Same ID, different metadata\n",
    "elem3 = Element(metadata={\"value\": 1})  # Different ID, same metadata\n",
    "\n",
    "print(f\"elem1 == elem2 (same ID, different metadata): {elem1 == elem2}\")\n",
    "print(f\"elem1 == elem3 (different ID, same metadata): {elem1 == elem3}\")\n",
    "print(f\"hash(elem1) == hash(elem2): {hash(elem1) == hash(elem2)}\")\n",
    "print(f\"hash(elem1) == hash(elem3): {hash(elem1) == hash(elem3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Immutability - Frozen by Default\n",
    "\n",
    "HashableModel is frozen to prevent hash corruption. If an object's hash changes while in a set/dict, lookups break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.375486Z",
     "iopub.status.busy": "2025-11-23T18:54:28.375345Z",
     "iopub.status.idle": "2025-11-23T18:54:28.379690Z",
     "shell.execute_reply": "2025-11-23T18:54:28.378633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cannot modify frozen model: ValidationError\n",
      "  1 validation error for Config\n",
      "temperature\n",
      "  Instance is frozen [type=frozen_instance, input_value=0.9, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/frozen_instance\n",
      "\n",
      "✓ Safe in sets: True\n"
     ]
    }
   ],
   "source": [
    "config = Config(name=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "try:\n",
    "    config.temperature = 0.9  # Attempt to modify\n",
    "    print(\"❌ Should not reach here\")\n",
    "except Exception as e:\n",
    "    print(f\"✓ Cannot modify frozen model: {type(e).__name__}\")\n",
    "    print(f\"  {e}\")\n",
    "\n",
    "# Why this matters:\n",
    "config_set = {config}\n",
    "print(f\"\\n✓ Safe in sets: {config in config_set}\")\n",
    "# If we could modify, the hash would change and lookup would fail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cache Key Usage\n",
    "\n",
    "Identical configurations produce the same hash, perfect for caching LLM calls or expensive computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.381442Z",
     "iopub.status.busy": "2025-11-23T18:54:28.381354Z",
     "iopub.status.idle": "2025-11-23T18:54:28.385566Z",
     "shell.execute_reply": "2025-11-23T18:54:28.384607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 1: Response for 'Hello' with gpt-4 @ temp=0.7\n",
      "Cache info: CacheInfo(hits=0, misses=1, maxsize=128, currsize=1)\n",
      "\n",
      "Call 2: Response for 'Hello' with gpt-4 @ temp=0.7\n",
      "Cache info: CacheInfo(hits=1, misses=1, maxsize=128, currsize=1)\n",
      "✓ Cache hit! (hits=1, misses=1)\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class LLMConfig(HashableModel):\n",
    "    model: str\n",
    "    temperature: float\n",
    "    max_tokens: int\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def expensive_llm_call(config: LLMConfig, prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM call - cached by config + prompt.\"\"\"\n",
    "    return f\"Response for '{prompt}' with {config.model} @ temp={config.temperature}\"\n",
    "\n",
    "\n",
    "# Same config instances\n",
    "cfg1 = LLMConfig(model=\"gpt-4\", temperature=0.7, max_tokens=1000)\n",
    "cfg2 = LLMConfig(model=\"gpt-4\", temperature=0.7, max_tokens=1000)\n",
    "\n",
    "# First call\n",
    "result1 = expensive_llm_call(cfg1, \"Hello\")\n",
    "print(f\"Call 1: {result1}\")\n",
    "print(f\"Cache info: {expensive_llm_call.cache_info()}\")\n",
    "\n",
    "# Second call with identical config - cache hit!\n",
    "result2 = expensive_llm_call(cfg2, \"Hello\")\n",
    "print(f\"\\nCall 2: {result2}\")\n",
    "print(f\"Cache info: {expensive_llm_call.cache_info()}\")\n",
    "print(\"✓ Cache hit! (hits=1, misses=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set and Dict Deduplication\n",
    "\n",
    "HashableModel instances deduplicate based on content when used in sets or as dict keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.388401Z",
     "iopub.status.busy": "2025-11-23T18:54:28.388090Z",
     "iopub.status.idle": "2025-11-23T18:54:28.392598Z",
     "shell.execute_reply": "2025-11-23T18:54:28.391733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list: 5 configs\n",
      "  0: name='gpt-4' temperature=0.7 max_tokens=1000\n",
      "  1: name='gpt-4' temperature=0.7 max_tokens=1000\n",
      "  2: name='gpt-4' temperature=0.9 max_tokens=1000\n",
      "  3: name='claude' temperature=0.7 max_tokens=1000\n",
      "  4: name='gpt-4' temperature=0.7 max_tokens=1000\n",
      "\n",
      "Unique configs: 3 configs\n",
      "  name='claude' temperature=0.7 max_tokens=1000\n",
      "  name='gpt-4' temperature=0.7 max_tokens=1000\n",
      "  name='gpt-4' temperature=0.9 max_tokens=1000\n",
      "\n",
      "Usage counts:\n",
      "  name='gpt-4' temperature=0.7 max_tokens=1000: used 3 times\n",
      "  name='gpt-4' temperature=0.9 max_tokens=1000: used 1 times\n",
      "  name='claude' temperature=0.7 max_tokens=1000: used 1 times\n"
     ]
    }
   ],
   "source": [
    "# Create multiple configs with duplicate values\n",
    "configs = [\n",
    "    Config(name=\"gpt-4\", temperature=0.7),\n",
    "    Config(name=\"gpt-4\", temperature=0.7),  # Duplicate\n",
    "    Config(name=\"gpt-4\", temperature=0.9),\n",
    "    Config(name=\"claude\", temperature=0.7),\n",
    "    Config(name=\"gpt-4\", temperature=0.7),  # Another duplicate\n",
    "]\n",
    "\n",
    "print(f\"Original list: {len(configs)} configs\")\n",
    "for i, cfg in enumerate(configs):\n",
    "    print(f\"  {i}: {cfg}\")\n",
    "\n",
    "# Deduplicate with set\n",
    "unique_configs = set(configs)\n",
    "print(f\"\\nUnique configs: {len(unique_configs)} configs\")\n",
    "for cfg in unique_configs:\n",
    "    print(f\"  {cfg}\")\n",
    "\n",
    "# Use as dict keys\n",
    "config_usage = {}\n",
    "for cfg in configs:\n",
    "    config_usage[cfg] = config_usage.get(cfg, 0) + 1\n",
    "\n",
    "print(\"\\nUsage counts:\")\n",
    "for cfg, count in config_usage.items():\n",
    "    print(f\"  {cfg}: used {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with to_list(unique=True)\n",
    "\n",
    "HashableModel works seamlessly with `to_list(unique=True)` for deduplicating structured LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.394828Z",
     "iopub.status.busy": "2025-11-23T18:54:28.394738Z",
     "iopub.status.idle": "2025-11-23T18:54:28.398988Z",
     "shell.execute_reply": "2025-11-23T18:54:28.398327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLM outputs: 5 tasks\n",
      "  - Write docs (high, 4h)\n",
      "  - Fix bug (medium, 2h)\n",
      "  - Write docs (high, 4h)\n",
      "  - Add tests (high, 3h)\n",
      "  - Fix bug (medium, 2h)\n",
      "\n",
      "Dedup with to_list(flatten=True, unique=True): 3 tasks\n",
      "  - Write docs (high, 4h)\n",
      "  - Fix bug (medium, 2h)\n",
      "  - Add tests (high, 3h)\n"
     ]
    }
   ],
   "source": [
    "class Task(HashableModel):\n",
    "    \"\"\"Structured task output from LLM.\"\"\"\n",
    "\n",
    "    title: str\n",
    "    priority: str\n",
    "    estimate_hours: int\n",
    "\n",
    "\n",
    "# Simulate LLM returning duplicate tasks\n",
    "llm_outputs = [\n",
    "    Task(title=\"Write docs\", priority=\"high\", estimate_hours=4),\n",
    "    Task(title=\"Fix bug\", priority=\"medium\", estimate_hours=2),\n",
    "    Task(title=\"Write docs\", priority=\"high\", estimate_hours=4),  # Duplicate\n",
    "    Task(title=\"Add tests\", priority=\"high\", estimate_hours=3),\n",
    "    Task(title=\"Fix bug\", priority=\"medium\", estimate_hours=2),  # Duplicate\n",
    "]\n",
    "\n",
    "print(f\"Raw LLM outputs: {len(llm_outputs)} tasks\")\n",
    "for task in llm_outputs:\n",
    "    print(f\"  - {task.title} ({task.priority}, {task.estimate_hours}h)\")\n",
    "\n",
    "# Deduplicate with to_list\n",
    "unique_tasks = to_list(llm_outputs, flatten=True, unique=True)\n",
    "print(f\"\\nDedup with to_list(flatten=True, unique=True): {len(unique_tasks)} tasks\")\n",
    "for task in unique_tasks:\n",
    "    print(f\"  - {task.title} ({task.priority}, {task.estimate_hours}h)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Serialization Modes\n",
    "\n",
    "Two modes: `python` (native types) and `json` (JSON-safe strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.401453Z",
     "iopub.status.busy": "2025-11-23T18:54:28.401180Z",
     "iopub.status.idle": "2025-11-23T18:54:28.406453Z",
     "shell.execute_reply": "2025-11-23T18:54:28.405526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python mode:\n",
      "  {'name': 'gpt-4', 'temperature': 0.7, 'max_tokens': 1500}\n",
      "  Types: [('name', 'str'), ('temperature', 'float'), ('max_tokens', 'int')]\n",
      "\n",
      "JSON mode:\n",
      "  {'max_tokens': 1500, 'name': 'gpt-4', 'temperature': 0.7}\n",
      "  Types: [('max_tokens', 'int'), ('name', 'str'), ('temperature', 'float')]\n",
      "\n",
      "JSON string: {\"max_tokens\":1500,\"name\":\"gpt-4\",\"temperature\":0.7}\n",
      "  Type: str\n"
     ]
    }
   ],
   "source": [
    "config = Config(name=\"gpt-4\", temperature=0.7, max_tokens=1500)\n",
    "\n",
    "# Python mode - native types\n",
    "python_dict = config.to_dict(mode=\"python\")\n",
    "print(\"Python mode:\")\n",
    "print(f\"  {python_dict}\")\n",
    "print(f\"  Types: {[(k, type(v).__name__) for k, v in python_dict.items()]}\")\n",
    "\n",
    "# JSON mode - JSON-safe types\n",
    "json_dict = config.to_dict(mode=\"json\")\n",
    "print(\"\\nJSON mode:\")\n",
    "print(f\"  {json_dict}\")\n",
    "print(f\"  Types: {[(k, type(v).__name__) for k, v in json_dict.items()]}\")\n",
    "\n",
    "# to_json - deterministic JSON string (sorted keys for stable hashing)\n",
    "json_str = config.to_json()\n",
    "print(f\"\\nJSON string: {json_str}\")\n",
    "print(f\"  Type: {type(json_str).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Roundtrip Fidelity\n",
    "\n",
    "Serialization and deserialization preserve values across both modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.408660Z",
     "iopub.status.busy": "2025-11-23T18:54:28.408529Z",
     "iopub.status.idle": "2025-11-23T18:54:28.412904Z",
     "shell.execute_reply": "2025-11-23T18:54:28.411583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python roundtrip:\n",
      "  Original:  name='gpt-4' temperature=0.85 max_tokens=2000\n",
      "  Restored:  name='gpt-4' temperature=0.85 max_tokens=2000\n",
      "  Equal: True\n",
      "  Same hash: True\n",
      "\n",
      "JSON roundtrip:\n",
      "  Original:  name='gpt-4' temperature=0.85 max_tokens=2000\n",
      "  Restored:  name='gpt-4' temperature=0.85 max_tokens=2000\n",
      "  Equal: True\n",
      "  Same hash: True\n",
      "\n",
      "from_json roundtrip:\n",
      "  Equal: True\n",
      "  Same hash: True\n"
     ]
    }
   ],
   "source": [
    "original = Config(name=\"gpt-4\", temperature=0.85, max_tokens=2000)\n",
    "\n",
    "# Python mode roundtrip\n",
    "python_data = original.to_dict(mode=\"python\")\n",
    "python_restored = Config.from_dict(python_data, mode=\"python\")\n",
    "print(\"Python roundtrip:\")\n",
    "print(f\"  Original:  {original}\")\n",
    "print(f\"  Restored:  {python_restored}\")\n",
    "print(f\"  Equal: {original == python_restored}\")\n",
    "print(f\"  Same hash: {hash(original) == hash(python_restored)}\")\n",
    "\n",
    "# JSON mode roundtrip\n",
    "json_data = original.to_dict(mode=\"json\")\n",
    "json_restored = Config.from_dict(json_data, mode=\"json\")\n",
    "print(\"\\nJSON roundtrip:\")\n",
    "print(f\"  Original:  {original}\")\n",
    "print(f\"  Restored:  {json_restored}\")\n",
    "print(f\"  Equal: {original == json_restored}\")\n",
    "print(f\"  Same hash: {hash(original) == hash(json_restored)}\")\n",
    "\n",
    "# from_json convenience method\n",
    "json_str = original.to_json()\n",
    "json_method_restored = Config.from_json(json_str)\n",
    "print(\"\\nfrom_json roundtrip:\")\n",
    "print(f\"  Equal: {original == json_method_restored}\")\n",
    "print(f\"  Same hash: {hash(original) == hash(json_method_restored)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hash Stability and Determinism\n",
    "\n",
    "Hash computation uses sorted JSON serialization for deterministic hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.415289Z",
     "iopub.status.busy": "2025-11-23T18:54:28.415159Z",
     "iopub.status.idle": "2025-11-23T18:54:28.418435Z",
     "shell.execute_reply": "2025-11-23T18:54:28.417895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash values:\n",
      "  config0: 4822402183230982788\n",
      "  config1: 4822402183230982788\n",
      "  config2: 4822402183230982788\n",
      "  config3: 4822402183230982788\n",
      "  config4: 4822402183230982788\n",
      "\n",
      "✓ All hashes identical: True\n",
      "✓ All JSON strings identical: True\n",
      "\n",
      "JSON: {\"max_tokens\":1000,\"name\":\"gpt-4\",\"temperature\":0.7}\n"
     ]
    }
   ],
   "source": [
    "# Create same config multiple times\n",
    "configs = [Config(name=\"gpt-4\", temperature=0.7, max_tokens=1000) for _ in range(5)]\n",
    "\n",
    "# All should have identical hashes\n",
    "hashes = [hash(cfg) for cfg in configs]\n",
    "print(\"Hash values:\")\n",
    "for i, h in enumerate(hashes):\n",
    "    print(f\"  config{i}: {h}\")\n",
    "\n",
    "print(f\"\\n✓ All hashes identical: {len(set(hashes)) == 1}\")\n",
    "\n",
    "# JSON determinism\n",
    "json_strings = [cfg.to_json() for cfg in configs]\n",
    "print(f\"✓ All JSON strings identical: {len(set(json_strings)) == 1}\")\n",
    "print(f\"\\nJSON: {json_strings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. When to Use HashableModel vs Element\n",
    "\n",
    "**Use HashableModel when:**\n",
    "- Value equality matters (identical configs should be same)\n",
    "- Caching based on configuration\n",
    "- Deduplicating structured LLM outputs\n",
    "- Immutable configuration objects\n",
    "- Set operations based on content\n",
    "\n",
    "**Use Element when:**\n",
    "- Identity matters (workflow entities)\n",
    "- Entities mutate over time (ID remains stable)\n",
    "- Tracking state changes\n",
    "- Building workflows/graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:54:28.420287Z",
     "iopub.status.busy": "2025-11-23T18:54:28.420208Z",
     "iopub.status.idle": "2025-11-23T18:54:28.424419Z",
     "shell.execute_reply": "2025-11-23T18:54:28.423811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HashableModel (Value Equality) ===\n",
      "Same values → same object: True\n",
      "Can deduplicate in sets: 1 unique\n",
      "\n",
      "=== Element (Identity-Based) ===\n",
      "Same content → different entities: False\n",
      "Both tracked separately: 2 unique\n",
      "\n",
      "Task 1 can evolve:\n",
      "  task1.metadata: {'status': 'complete'}\n",
      "  task2.metadata: {}\n",
      "  Still same ID-based identity: True\n"
     ]
    }
   ],
   "source": [
    "# HashableModel: Value equality for configs\n",
    "class APIConfig(HashableModel):\n",
    "    endpoint: str\n",
    "    timeout: int\n",
    "    retries: int\n",
    "\n",
    "\n",
    "cfg1 = APIConfig(endpoint=\"/api/v1\", timeout=30, retries=3)\n",
    "cfg2 = APIConfig(endpoint=\"/api/v1\", timeout=30, retries=3)\n",
    "print(\"=== HashableModel (Value Equality) ===\")\n",
    "print(f\"Same values → same object: {cfg1 == cfg2}\")\n",
    "print(f\"Can deduplicate in sets: {len({cfg1, cfg2})} unique\")\n",
    "\n",
    "# Element: Identity-based for workflow entities\n",
    "from lionpride.core import Node\n",
    "\n",
    "task1 = Node(content={\"task\": \"Write docs\"})\n",
    "task2 = Node(content={\"task\": \"Write docs\"})  # Same content, different entity\n",
    "print(\"\\n=== Element (Identity-Based) ===\")\n",
    "print(f\"Same content → different entities: {task1 == task2}\")\n",
    "print(f\"Both tracked separately: {len({task1, task2})} unique\")\n",
    "print(\"\\nTask 1 can evolve:\")\n",
    "task1.metadata[\"status\"] = \"complete\"\n",
    "print(f\"  task1.metadata: {task1.metadata}\")\n",
    "print(f\"  task2.metadata: {task2.metadata}\")\n",
    "print(f\"  Still same ID-based identity: {task1.id == task1.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "**HashableModel Essentials:**\n",
    "- ✅ Content-based equality (identical fields → same hash)\n",
    "- ✅ Immutable by default (frozen to prevent hash corruption)\n",
    "- ✅ Safe for sets, dicts, and `to_list(unique=True)`\n",
    "- ✅ Perfect for cache keys (identical configs → same hash)\n",
    "- ✅ Deterministic JSON serialization (sorted keys)\n",
    "- ✅ Two serialization modes: `python` and `json`\n",
    "- ✅ Lossless roundtrip through serialization\n",
    "\n",
    "**Key Difference from Element:**\n",
    "- HashableModel: Value equality (same fields = equal)\n",
    "- Element: Identity equality (same ID = equal)\n",
    "\n",
    "**Use Cases:**\n",
    "- Configuration objects for caching\n",
    "- Structured LLM output deduplication\n",
    "- Set operations based on content\n",
    "- Immutable data structures\n",
    "\n",
    "**Next Steps:**\n",
    "- See `Element` for identity-based entities\n",
    "- See `Node` for mutable workflow entities\n",
    "- See `to_list` for collection utilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lionpride)",
   "language": "python",
   "name": "lionpride"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
