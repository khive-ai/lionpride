{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LNDL v2: Live API Testing\n",
    "\n",
    "This notebook demonstrates LNDL (Lion's Natural Directive Language) v2 with live API calls.\n",
    "\n",
    "**Goals:**\n",
    "1. Test LNDL parsing with real model responses\n",
    "2. Validate structured output generation\n",
    "3. Demonstrate the Operative pattern with actions\n",
    "4. Test streaming output\n",
    "\n",
    "**Requirements:**\n",
    "- Set `ANTHROPIC_API_KEY` or `OPENAI_API_KEY` environment variable\n",
    "- lionpride package installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src to path for development\n",
    "sys.path.insert(0, os.path.abspath(\"../src\"))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Check for API keys\n",
    "has_anthropic = bool(os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "has_openai = bool(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(f\"Anthropic API available: {has_anthropic}\")\n",
    "print(f\"OpenAI API available: {has_openai}\")\n",
    "\n",
    "if not has_anthropic and not has_openai:\n",
    "    print(\"\\nWarning: No API key found. Set ANTHROPIC_API_KEY or OPENAI_API_KEY to run live tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup\n",
    "\n",
    "Import core components and create a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from lionpride.lndl import parse_lndl_fuzzy\n",
    "from lionpride.operations import create_action_operative, create_operative_from_model\n",
    "from lionpride.operations.lndl import generate_lndl_spec_format, prepare_lndl_messages\n",
    "from lionpride.operations.validation import validate_response\n",
    "from lionpride.services import iModel\n",
    "from lionpride.services.providers.oai_chat import OAIChatEndpoint\n",
    "from lionpride.session import Session\n",
    "\n",
    "# Create session\n",
    "session = Session()\n",
    "branch = session.create_branch()\n",
    "\n",
    "print(f\"Session created: {session.id}\")\n",
    "print(f\"Branch created: {branch.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Response Models\n",
    "\n",
    "Define Pydantic models for structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisResult(BaseModel):\n",
    "    \"\"\"Structured analysis result.\"\"\"\n",
    "\n",
    "    topic: str = Field(..., description=\"Topic analyzed\")\n",
    "    summary: str = Field(..., description=\"Brief summary of findings\")\n",
    "    key_points: list[str] = Field(default_factory=list, description=\"Key points\")\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence score\")\n",
    "\n",
    "\n",
    "class CodeReview(BaseModel):\n",
    "    \"\"\"Code review result.\"\"\"\n",
    "\n",
    "    file_path: str = Field(..., description=\"File being reviewed\")\n",
    "    issues: list[str] = Field(default_factory=list, description=\"Issues found\")\n",
    "    suggestions: list[str] = Field(default_factory=list, description=\"Improvement suggestions\")\n",
    "    quality_score: float = Field(..., ge=0.0, le=10.0, description=\"Code quality score\")\n",
    "    approve: bool = Field(..., description=\"Whether to approve\")\n",
    "\n",
    "\n",
    "# Create operatives\n",
    "analysis_operative = create_operative_from_model(AnalysisResult, name=\"Analysis\")\n",
    "review_operative = create_operative_from_model(CodeReview, name=\"CodeReview\")\n",
    "\n",
    "print(\"Analysis Operative:\")\n",
    "print(f\"  - Operable: {analysis_operative.operable}\")\n",
    "print(f\"  - Specs: {[s.name for s in analysis_operative.operable.get_specs()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate LNDL Prompt\n",
    "\n",
    "See the LNDL format specification that guides the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lndl_prompt = generate_lndl_spec_format(analysis_operative)\n",
    "print(\"LNDL Spec Format:\")\n",
    "print(\"-\" * 60)\n",
    "print(lndl_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parse LNDL Response (Mock)\n",
    "\n",
    "Test LNDL parsing with a mock response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate model response in LNDL format\n",
    "mock_lndl_response = \"\"\"<lvar AnalysisResult.topic t>Machine Learning Fundamentals</lvar>\n",
    "<lvar AnalysisResult.summary s>An overview of core ML concepts including supervised learning, neural networks, and optimization techniques.</lvar>\n",
    "<lvar AnalysisResult.key_points kp>[\"Supervised vs unsupervised learning\", \"Neural network architectures\", \"Gradient descent optimization\", \"Model evaluation metrics\"]</lvar>\n",
    "<lvar AnalysisResult.confidence c>0.92</lvar>\n",
    "OUT{analysis: [t, s, kp, c]}\"\"\"\n",
    "\n",
    "print(\"Mock LNDL Response:\")\n",
    "print(mock_lndl_response)\n",
    "print()\n",
    "\n",
    "# Parse the LNDL\n",
    "parsed = parse_lndl_fuzzy(mock_lndl_response, analysis_operative.operable)\n",
    "print(\"Parsed Result:\")\n",
    "print(f\"  Topic: {parsed.fields.get('analysis').topic}\")\n",
    "print(f\"  Summary: {parsed.fields.get('analysis').summary}\")\n",
    "print(f\"  Key Points: {parsed.fields.get('analysis').key_points}\")\n",
    "print(f\"  Confidence: {parsed.fields.get('analysis').confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation Strategy\n",
    "\n",
    "Test the validation strategy pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test validation with different responses\n",
    "\n",
    "# Valid LNDL response\n",
    "valid_lndl = \"\"\"<lvar AnalysisResult.topic t>Python Best Practices</lvar>\n",
    "<lvar AnalysisResult.summary s>Guidelines for writing clean Python code.</lvar>\n",
    "<lvar AnalysisResult.key_points kp>[\"Use type hints\", \"Write docstrings\"]</lvar>\n",
    "<lvar AnalysisResult.confidence c>0.85</lvar>\n",
    "OUT{analysis: [t, s, kp, c]}\"\"\"\n",
    "\n",
    "result = validate_response(\n",
    "    valid_lndl,\n",
    "    operable=analysis_operative,\n",
    "    threshold=0.6,\n",
    ")\n",
    "\n",
    "print(f\"Validation Success: {result.success}\")\n",
    "if result.success:\n",
    "    print(f\"Data: {result.data}\")\n",
    "else:\n",
    "    print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Action Operative\n",
    "\n",
    "Create an operative with tool/action support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create operative with actions\n",
    "action_operative = create_action_operative(\n",
    "    base_model=AnalysisResult,\n",
    "    reason=True,\n",
    "    actions=True,\n",
    "    name=\"AnalysisWithActions\",\n",
    ")\n",
    "\n",
    "print(\"Action Operative Specs:\")\n",
    "for spec in action_operative.operable.get_specs():\n",
    "    print(f\"  - {spec.name}: {spec.base_type}\")\n",
    "\n",
    "print(f\"\\nSupports Actions: {action_operative._supports_actions}\")\n",
    "print(f\"Request Exclude: {action_operative.request_exclude}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Live API Test (Optional)\n",
    "\n",
    "Test with real API if key is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if no API key\n",
    "if not has_anthropic and not has_openai:\n",
    "    print(\"Skipping live API test - no API key available\")\n",
    "else:\n",
    "    # Configure endpoint based on available API key\n",
    "    if has_openai:\n",
    "        endpoint = OAIChatEndpoint(model=\"gpt-4o-mini\")\n",
    "        model = iModel(backend=endpoint, name=\"openai_chat\")\n",
    "    else:\n",
    "        # For Anthropic, would need AnthropicEndpoint (not shown here)\n",
    "        print(\"Anthropic endpoint not configured in this demo\")\n",
    "        model = None\n",
    "\n",
    "    if model:\n",
    "        session.services.register(model)\n",
    "        print(f\"Registered model: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live test with LNDL format\n",
    "if has_openai and model:\n",
    "    from lionpride.operations.lndl import generate_lndl_spec_format, prepare_lndl_messages\n",
    "    from lionpride.session.messages import InstructionContent, Message\n",
    "\n",
    "    # Create operative for LNDL\n",
    "    operative = create_operative_from_model(AnalysisResult, name=\"Analysis\")\n",
    "\n",
    "    # Show the LNDL spec format\n",
    "    print(\"=== LNDL Spec Format ===\")\n",
    "    print(generate_lndl_spec_format(operative))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # IMPORTANT: Don't pass response_model to InstructionContent for LNDL mode\n",
    "    # The LNDL spec is in the system message, not the user message\n",
    "    ins_content = InstructionContent.create(\n",
    "        instruction=\"Analyze the topic 'Python async/await patterns'. Provide a structured analysis.\",\n",
    "        # NO response_model - LNDL format guidance is in system message\n",
    "    )\n",
    "    ins_msg = Message(content=ins_content, sender=\"user\", recipient=\"assistant\")\n",
    "\n",
    "    # Prepare LNDL messages (injects LNDL system prompt + spec)\n",
    "    messages = prepare_lndl_messages(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        ins_msg=ins_msg,\n",
    "        operable=operative.operable,\n",
    "    )\n",
    "\n",
    "    # Invoke the model\n",
    "    print(\"\\nInvoking model...\")\n",
    "    calling = await model.invoke(messages=messages, max_tokens=1000)\n",
    "    response_text = calling.execution.response.data\n",
    "\n",
    "    print(\"\\n=== Model Response ===\")\n",
    "    print(response_text)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Parse LNDL response\n",
    "    print(\"\\n=== Parsing LNDL ===\")\n",
    "    try:\n",
    "        parsed = parse_lndl_fuzzy(response_text, operative.operable)\n",
    "        analysis = parsed.fields.get(\"analysis\")\n",
    "\n",
    "        print(f\"✓ Topic: {analysis.topic}\")\n",
    "        print(f\"✓ Summary: {analysis.summary}\")\n",
    "        print(f\"✓ Key Points: {analysis.key_points}\")\n",
    "        print(f\"✓ Confidence: {analysis.confidence}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Parse error: {e}\")\n",
    "else:\n",
    "    print(\"Skipping live API test - no OpenAI API key or model not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming Test (Optional)\n",
    "\n",
    "Test streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming test using StreamChannel (handles SSE parsing automatically)\n",
    "if has_openai and model:\n",
    "    print(\"Streaming response via StreamChannel:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # invoke_stream_with_channel wraps invoke_stream in StreamChannel\n",
    "    # which automatically parses SSE and extracts content\n",
    "    channel = await model.invoke_stream_with_channel(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Count from 1 to 5, one number per line.\"}],\n",
    "        max_tokens=50,\n",
    "    )\n",
    "\n",
    "    # Add a consumer that prints each chunk as it arrives\n",
    "    channel.add_consumer(\n",
    "        lambda chunk: print(chunk.content, end=\"\", flush=True) if not chunk.is_final else None\n",
    "    )\n",
    "\n",
    "    # Iterate through the channel (this drives the stream)\n",
    "    async for _chunk in channel:\n",
    "        pass  # Consumer handles printing\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(f\"Full accumulated text: {channel.get_accumulated()}\")\n",
    "    print(f\"Total chunks received: {len(channel.get_buffer())}\")\n",
    "else:\n",
    "    print(\"Skipping streaming test - no OpenAI API key or model not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling\n",
    "\n",
    "Test LNDL error cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionpride.lndl.errors import MissingOutBlockError, UnresolvedAliasError\n",
    "\n",
    "# Test missing OUT block\n",
    "malformed_lndl = \"\"\"<lvar AnalysisResult.topic t>Test</lvar>\n",
    "<lvar AnalysisResult.summary s>Summary</lvar>\"\"\"\n",
    "\n",
    "try:\n",
    "    parse_lndl_fuzzy(malformed_lndl, analysis_operative.operable)\n",
    "except MissingOutBlockError as e:\n",
    "    print(f\"Expected error caught: {e}\")\n",
    "\n",
    "# Test unresolved alias\n",
    "bad_alias_lndl = \"\"\"<lvar AnalysisResult.topic t>Test</lvar>\n",
    "OUT{analysis: [t, unknown_alias]}\"\"\"\n",
    "\n",
    "try:\n",
    "    parse_lndl_fuzzy(bad_alias_lndl, analysis_operative.operable)\n",
    "except UnresolvedAliasError as e:\n",
    "    print(f\"Expected error caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 10. Summary\n\nLNDL v2 features demonstrated:\n\n1. **Operative Pattern** - Wrap Pydantic models with validation support\n2. **LNDL Format Generation** - `operations/lndl/formatting.py` generates spec format\n3. **Message Preparation** - `prepare_lndl_messages()` composes system + user messages\n4. **Fuzzy Parsing** - Flexible parsing with alias resolution in `lndl/` module\n5. **Validation Strategy** - Configurable validation with thresholds\n6. **Action Support** - Tool/action integration for agentic workflows\n7. **Streaming** - Async streaming with channel abstraction\n8. **Error Handling** - Typed errors for debugging\n\n**Architecture:**\n- `InstructionContent` handles JSON format (with response_model)\n- `operations/lndl/` handles LNDL format (via prepare_lndl_messages)\n- `lndl/` module handles core parsing (parse_lndl_fuzzy)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LNDL v2 Demo Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionpride",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
