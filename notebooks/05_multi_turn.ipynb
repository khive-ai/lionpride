{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Turn Conversations\n",
    "\n",
    "Learn how to manage conversation history, context, and branching with lionpride's Branch system.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- How Branch tracks conversation history\n",
    "- Building multi-turn conversations with `communicate()`\n",
    "- Using system messages for persona/behavior\n",
    "- Forking conversations for \"what if\" scenarios\n",
    "- Managing conversation context and persistence\n",
    "\n",
    "**Prerequisites:** Basic understanding of Session and Message from earlier notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionpride import Message, Session\n",
    "from lionpride.operations import communicate\n",
    "from lionpride.services import iModel\n",
    "from lionpride.session import SystemContent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Branch Structure\n",
    "\n",
    "A **Branch** is a conversation thread that maintains message history:\n",
    "\n",
    "```python\n",
    "Branch\n",
    "├── order: list[UUID]        # Message IDs in chronological order\n",
    "├── system: UUID | None      # System message (first in order)\n",
    "├── capabilities: set[str]   # Allowed structured outputs\n",
    "├── resources: set[str]      # Allowed service resources\n",
    "└── session_id: UUID         # Parent session\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- Branch stores **UUID references**, actual messages live in `session.messages`\n",
    "- Messages appear in chronological order via `branch.order`\n",
    "- System message (if any) is always first\n",
    "- Multiple branches can exist in one session (parallel conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session and inspect a branch\n",
    "session = Session()\n",
    "branch = session.create_branch(name=\"demo\")\n",
    "\n",
    "print(f\"Branch ID: {branch.id}\")\n",
    "print(f\"Branch name: {branch.name}\")\n",
    "print(f\"Messages in branch: {len(branch.order)}\")\n",
    "print(f\"System message: {branch.system}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Multi-Turn Chat\n",
    "\n",
    "Each call to `communicate()` adds messages to the branch. The model sees the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def basic_multi_turn():\n",
    "    \"\"\"Simple multi-turn conversation with automatic context\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"python-help\")\n",
    "\n",
    "    # Turn 1: Initial question\n",
    "    response1 = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"What are Python lists?\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "    print(\"Turn 1:\")\n",
    "    print(\"User: What are Python lists?\")\n",
    "    print(f\"Assistant: {response1[:200]}...\\n\")\n",
    "\n",
    "    # Turn 2: Follow-up (has context from turn 1)\n",
    "    response2 = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"How do I add items to them?\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "    print(\"Turn 2:\")\n",
    "    print(\"User: How do I add items to them?\")  # Notice the pronoun \"them\"\n",
    "    print(f\"Assistant: {response2[:200]}...\\n\")\n",
    "\n",
    "    # Turn 3: Another follow-up\n",
    "    response3 = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Show me an example with numbers\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "    print(\"Turn 3:\")\n",
    "    print(\"User: Show me an example with numbers\")\n",
    "    print(f\"Assistant: {response3[:200]}...\\n\")\n",
    "\n",
    "    # Inspect conversation\n",
    "    print(f\"Total messages in branch: {len(branch.order)}\")\n",
    "    print(f\"Message IDs: {[str(mid)[:8] + '...' for mid in branch.order]}\")\n",
    "\n",
    "    return session, branch\n",
    "\n",
    "\n",
    "# Run the conversation\n",
    "session, branch = await basic_multi_turn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: In turn 2, we used \"them\" (pronoun reference) - the model understood from context!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting Message History\n",
    "\n",
    "Access messages via `session.messages[msg_id]` using IDs from `branch.order`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect each message in the conversation\n",
    "print(\"=== Conversation History ===\")\n",
    "print()\n",
    "\n",
    "for i, msg_id in enumerate(branch.order, 1):\n",
    "    message = session.messages[msg_id]\n",
    "    role = message.role.value\n",
    "\n",
    "    # Extract content based on message type\n",
    "    if hasattr(message.content, \"instruction\"):\n",
    "        content = message.content.instruction\n",
    "    elif hasattr(message.content, \"assistant_response\"):\n",
    "        content = message.content.assistant_response\n",
    "    else:\n",
    "        content = str(message.content)\n",
    "\n",
    "    print(f\"Message {i} [{role}]:\")\n",
    "    print(f\"  {content[:100]}...\")\n",
    "    print(f\"  Created: {message.created_at}\")\n",
    "    print(f\"  ID: {message.id}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. System Messages and Personas\n",
    "\n",
    "System messages define the model's behavior/personality. They're always the first message in a branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def persona_conversation():\n",
    "    \"\"\"Demonstrate system message influence on behavior\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    # Create system message for a Python tutor persona\n",
    "    # Note: SystemContent uses 'system_message' field, not 'system'\n",
    "    system_msg = Message(\n",
    "        content=SystemContent(\n",
    "            system_message=\"You are an enthusiastic Python tutor who loves using \"\n",
    "            \"emojis and encourages students. Keep responses concise \"\n",
    "            \"and always include a code example.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    branch = session.create_branch(name=\"tutoring\", system=system_msg)\n",
    "\n",
    "    # Ask a question\n",
    "    response = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"What are dictionaries?\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"With enthusiastic tutor persona:\")\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "    # Verify system message is first\n",
    "    first_msg = session.messages[branch.order[0]]\n",
    "    print(f\"First message role: {first_msg.role.value}\")\n",
    "    print(f\"System message ID matches: {branch.system == first_msg.id}\")\n",
    "\n",
    "    return session, branch\n",
    "\n",
    "\n",
    "await persona_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: The response style matches the system message's personality!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Branches in One Session\n",
    "\n",
    "Different branches maintain independent conversation threads with separate contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def multiple_branches_demo():\n",
    "    \"\"\"Parallel conversations in different branches\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    # Branch 1: Technical discussion\n",
    "    tech_system = Message(\n",
    "        content=SystemContent(\n",
    "            system_message=\"You are a senior software engineer discussing architecture. \"\n",
    "            \"Be precise and technical.\"\n",
    "        )\n",
    "    )\n",
    "    tech_branch = session.create_branch(name=\"technical\", system=tech_system)\n",
    "\n",
    "    # Branch 2: Creative writing\n",
    "    creative_system = Message(\n",
    "        content=SystemContent(\n",
    "            system_message=\"You are a creative writing coach. Be imaginative and \"\n",
    "            \"encourage vivid storytelling.\"\n",
    "        )\n",
    "    )\n",
    "    creative_branch = session.create_branch(name=\"creative\", system=creative_system)\n",
    "\n",
    "    # Parallel conversations\n",
    "    tech_response = await communicate(\n",
    "        session=session,\n",
    "        branch=tech_branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Explain microservices in 2 sentences\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    creative_response = await communicate(\n",
    "        session=session,\n",
    "        branch=creative_branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Write an opening line for a sci-fi story\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"Technical Branch:\")\n",
    "    print(tech_response)\n",
    "    print()\n",
    "\n",
    "    print(\"Creative Branch:\")\n",
    "    print(creative_response)\n",
    "    print()\n",
    "\n",
    "    # Continue technical conversation (no creative context)\n",
    "    tech_response2 = await communicate(\n",
    "        session=session,\n",
    "        branch=tech_branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"What are the main trade-offs?\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"Technical Branch (Turn 2):\")\n",
    "    print(tech_response2)\n",
    "    print()\n",
    "\n",
    "    # Session statistics\n",
    "    print(\"=== Session Statistics ===\")\n",
    "    print(f\"Total branches: {len(session.branches)}\")\n",
    "    print(f\"Total messages: {len(session.messages)}\")\n",
    "    print(f\"Technical branch messages: {len(tech_branch.order)}\")\n",
    "    print(f\"Creative branch messages: {len(creative_branch.order)}\")\n",
    "\n",
    "    return session\n",
    "\n",
    "\n",
    "await multiple_branches_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: Branches maintain independent context. The technical branch doesn't know about the creative conversation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Forking Conversations\n",
    "\n",
    "Create a new branch starting from a specific point in an existing conversation (\"what if\" scenarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def forking_demo():\n",
    "    \"\"\"Fork a conversation to explore alternatives\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    # Start a conversation\n",
    "    main_branch = session.create_branch(name=\"main\")\n",
    "\n",
    "    await communicate(\n",
    "        session=session,\n",
    "        branch=main_branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"I'm building a web app. Should I use REST or GraphQL?\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    response = await communicate(\n",
    "        session=session,\n",
    "        branch=main_branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Let's go with REST. What framework?\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"Main branch (REST path):\")\n",
    "    print(response[:200], \"...\\n\")\n",
    "\n",
    "    # Fork at the decision point (before choosing REST)\n",
    "    # Take first 2 messages (initial question + answer)\n",
    "    fork_point = 2\n",
    "    alternative_branch = session.create_branch(\n",
    "        name=\"graphql-alternative\", messages=main_branch.order[:fork_point]\n",
    "    )\n",
    "\n",
    "    # Explore alternative path\n",
    "    alt_response = await communicate(\n",
    "        session=session,\n",
    "        branch=alternative_branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Actually, let's explore GraphQL. What are the benefits?\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"Alternative branch (GraphQL path):\")\n",
    "    print(alt_response[:200], \"...\\n\")\n",
    "\n",
    "    # Compare branch states\n",
    "    print(\"=== Branch Comparison ===\")\n",
    "    print(f\"Main branch messages: {len(main_branch.order)}\")\n",
    "    print(f\"Alternative branch messages: {len(alternative_branch.order)}\")\n",
    "    print(\n",
    "        f\"Shared history (first {fork_point} messages): {main_branch.order[:fork_point] == alternative_branch.order[:fork_point]}\"\n",
    "    )\n",
    "\n",
    "    return session\n",
    "\n",
    "\n",
    "await forking_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Case**: A/B testing responses, exploring different solutions, or debugging conversation paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explicit Context Management\n",
    "\n",
    "Provide additional context alongside instructions using the `context` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def context_management():\n",
    "    \"\"\"Provide explicit context for more precise responses\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"code-review\")\n",
    "\n",
    "    # Turn 1: Provide code in context\n",
    "    response1 = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Review this code for bugs\",\n",
    "            \"context\": {\n",
    "                \"code\": \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "                \"\"\",\n",
    "                \"language\": \"Python\",\n",
    "                \"focus\": \"edge cases\",\n",
    "            },\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"Code Review:\")\n",
    "    print(response1)\n",
    "    print()\n",
    "\n",
    "    # Turn 2: Follow-up with additional context\n",
    "    response2 = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"How would you fix it?\",\n",
    "            \"context\": {\n",
    "                \"requirement\": \"Handle empty lists gracefully\",\n",
    "                \"style\": \"Use modern Python idioms\",\n",
    "            },\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"Fix Suggestion:\")\n",
    "    print(response2)\n",
    "\n",
    "    return session, branch\n",
    "\n",
    "\n",
    "await context_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Practice**: Use `context` for structured data (code, configs, specs) separate from the instruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conversation Persistence\n",
    "\n",
    "Save and resume conversations across sessions using serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "async def save_conversation():\n",
    "    \"\"\"Save conversation to disk\"\"\"\n",
    "    session = Session()\n",
    "    model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"persistent\")\n",
    "\n",
    "    # Have a conversation\n",
    "    await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"What are Python decorators?\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Show me a practical example\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Export conversation\n",
    "    conversation_data = {\n",
    "        \"branch_id\": str(branch.id),\n",
    "        \"branch_name\": branch.name,\n",
    "        \"messages\": [\n",
    "            {\"id\": str(msg_id), \"message\": session.messages[msg_id].to_dict()}\n",
    "            for msg_id in branch.order\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Save to file\n",
    "    save_path = Path(\"/tmp/conversation.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(conversation_data, f, default=str, indent=2)\n",
    "\n",
    "    print(f\"✓ Conversation saved to {save_path}\")\n",
    "    print(f\"  Messages: {len(branch.order)}\")\n",
    "    return save_path\n",
    "\n",
    "\n",
    "save_path = await save_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def resume_conversation(save_path):\n",
    "    \"\"\"Resume conversation from disk\"\"\"\n",
    "    # Create new session\n",
    "    new_session = Session()\n",
    "    new_model = iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    new_session.services.register(new_model)\n",
    "\n",
    "    # Load conversation\n",
    "    with open(save_path) as f:\n",
    "        loaded_data = json.load(f)\n",
    "\n",
    "    # Reconstruct messages\n",
    "    messages = []\n",
    "    for msg_data in loaded_data[\"messages\"]:\n",
    "        msg = Message.from_dict(msg_data[\"message\"])\n",
    "        messages.append(msg)\n",
    "        new_session.conversations.add_item(msg)  # Add to session\n",
    "\n",
    "    # Recreate branch with message references\n",
    "    new_branch = new_session.create_branch(\n",
    "        name=loaded_data[\"branch_name\"], messages=[msg.id for msg in messages]\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Resumed conversation: {new_branch.name}\")\n",
    "    print(f\"  Messages loaded: {len(new_branch.order)}\")\n",
    "    print()\n",
    "\n",
    "    # Continue conversation\n",
    "    response = await communicate(\n",
    "        session=new_session,\n",
    "        branch=new_branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Can you explain the example in more detail?\",\n",
    "            \"imodel\": new_model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"Continued conversation:\")\n",
    "    print(response[:300], \"...\")\n",
    "    print()\n",
    "    print(f\"Total messages now: {len(new_branch.order)}\")\n",
    "\n",
    "    return new_session, new_branch\n",
    "\n",
    "\n",
    "await resume_conversation(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Case**: Long-running conversations, session recovery, conversation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Managing Context Windows\n",
    "\n",
    "Limit message history to stay within model context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recent_messages(session, branch, n=5):\n",
    "    \"\"\"Get last N messages from a branch\"\"\"\n",
    "    recent_ids = branch.order[-n:]\n",
    "    return [session.messages[msg_id] for msg_id in recent_ids]\n",
    "\n",
    "\n",
    "# Example: Get last 3 messages\n",
    "session = Session()\n",
    "branch = session.create_branch(name=\"test\")\n",
    "\n",
    "# Simulate multiple messages\n",
    "for i in range(10):\n",
    "    msg = Message(content={\"instruction\": f\"Message {i}\"})\n",
    "    session.conversations.add_item(msg)\n",
    "    branch.order.append(msg.id)\n",
    "\n",
    "recent = get_recent_messages(session, branch, n=3)\n",
    "print(f\"Total messages: {len(branch.order)}\")\n",
    "print(f\"Recent messages: {len(recent)}\")\n",
    "print(f\"Recent IDs: {[str(m.id)[:8] + '...' for m in recent]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pattern**: Create a sliding window of recent messages for very long conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "**What You Learned:**\n",
    "- ✅ Branch tracks message history via `order: list[UUID]`\n",
    "- ✅ `communicate()` automatically builds multi-turn context\n",
    "- ✅ System messages define model behavior (always first in branch)\n",
    "- ✅ Multiple branches enable parallel conversations\n",
    "- ✅ Forking creates alternative conversation paths\n",
    "- ✅ `context` parameter provides structured data\n",
    "- ✅ Serialization enables conversation persistence\n",
    "- ✅ Sliding windows manage long conversations\n",
    "\n",
    "**Common Pitfalls:**\n",
    "- ❌ Don't modify `branch.order` directly - use `communicate()`\n",
    "- ❌ Don't forget to add messages to `session.conversations` first\n",
    "- ❌ Don't assume branches share context - they're independent\n",
    "- ❌ Don't forget system message is always `branch.order[0]`\n",
    "\n",
    "**Next Steps:**\n",
    "- See notebook 06: Tool calling with conversation context\n",
    "- See notebook 07: Streaming multi-turn responses\n",
    "- See notebook 08: Multi-agent conversations\n",
    "\n",
    "**Architecture Insight:**\n",
    "```python\n",
    "Session\n",
    "├── conversations: Flow[Message, Branch]\n",
    "│   ├── messages (Pile[Message])      # All messages\n",
    "│   └── branches (Pile[Branch])       # All threads\n",
    "└── services: ServiceRegistry\n",
    "    └── models, tools\n",
    "```\n",
    "\n",
    "Branch = Progression with conversation-specific methods. Messages = Nodes with chat-specific content types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
