{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Multiple Providers\n",
    "\n",
    "This notebook demonstrates how to use lionpride with different LLM providers. The API is **identical** across providers - you only change the `provider` and `model` parameters.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Same code works with any provider\n",
    "- Easy provider switching\n",
    "- Fallback chains for resilience\n",
    "- Provider-specific optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, set your API keys as environment variables (only for providers you want to use):\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "export GEMINI_API_KEY=\"...\"\n",
    "export GROQ_API_KEY=\"gsk_...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from lionpride import Session\n",
    "from lionpride.operations import communicate\n",
    "from lionpride.services import iModel\n",
    "\n",
    "# Verify at least one API key is set\n",
    "providers_available = {\n",
    "    \"openai\": bool(os.getenv(\"OPENAI_API_KEY\")),\n",
    "    \"anthropic\": bool(os.getenv(\"ANTHROPIC_API_KEY\")),\n",
    "    \"gemini\": bool(os.getenv(\"GEMINI_API_KEY\")),\n",
    "    \"groq\": bool(os.getenv(\"GROQ_API_KEY\")),\n",
    "}\n",
    "\n",
    "print(\"Available providers:\")\n",
    "for provider, available in providers_available.items():\n",
    "    status = \"✓\" if available else \"✗\"\n",
    "    print(f\"  {status} {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: OpenAI GPT-4o\n",
    "\n",
    "OpenAI provides the most widely-used models including GPT-4o and GPT-4o-mini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_openai():\n",
    "    \"\"\"Chat with OpenAI GPT-4o\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # OpenAI configuration\n",
    "    model = iModel(\n",
    "        provider=\"openai\",\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o\" for more capable model\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"openai-chat\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Explain quantum computing in simple terms\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"OpenAI Response:\\n{result}\\n\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run if OpenAI is available\n",
    "if providers_available[\"openai\"]:\n",
    "    await chat_openai()\n",
    "else:\n",
    "    print(\"⚠️ OpenAI not configured. Set OPENAI_API_KEY to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Anthropic Claude\n",
    "\n",
    "Anthropic's Claude models excel at long-context analysis and nuanced reasoning.\n",
    "\n",
    "**Important:** Anthropic requires the `endpoint=\"messages\"` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_anthropic():\n",
    "    \"\"\"Chat with Anthropic Claude\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Anthropic configuration - note the 'messages' endpoint\n",
    "    model = iModel(\n",
    "        provider=\"anthropic\",\n",
    "        endpoint=\"messages\",  # Important: Anthropic uses 'messages' endpoint\n",
    "        model=\"claude-3-5-haiku-20241022\",  # or \"claude-3-5-sonnet-20241022\"\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"claude-chat\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Explain quantum computing in simple terms\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Claude Response:\\n{result}\\n\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run if Anthropic is available\n",
    "if providers_available[\"anthropic\"]:\n",
    "    await chat_anthropic()\n",
    "else:\n",
    "    print(\"⚠️ Anthropic not configured. Set ANTHROPIC_API_KEY to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Google Gemini\n",
    "\n",
    "Google's Gemini models offer strong multimodal capabilities and a generous free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_gemini():\n",
    "    \"\"\"Chat with Google Gemini\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Gemini configuration\n",
    "    model = iModel(\n",
    "        provider=\"gemini\",\n",
    "        model=\"gemini-2.0-flash-exp\",  # or \"gemini-1.5-pro\" for larger model\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"gemini-chat\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Explain quantum computing in simple terms\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Gemini Response:\\n{result}\\n\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run if Gemini is available\n",
    "if providers_available[\"gemini\"]:\n",
    "    await chat_gemini()\n",
    "else:\n",
    "    print(\"⚠️ Gemini not configured. Set GEMINI_API_KEY to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Multi-Provider Comparison\n",
    "\n",
    "Ask the same question to multiple providers and compare responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compare_providers():\n",
    "    \"\"\"Compare responses from different providers\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Register multiple models\n",
    "    model_configs = []\n",
    "\n",
    "    if providers_available[\"openai\"]:\n",
    "        model_configs.append(iModel(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.7))\n",
    "\n",
    "    if providers_available[\"anthropic\"]:\n",
    "        model_configs.append(\n",
    "            iModel(\n",
    "                provider=\"anthropic\",\n",
    "                endpoint=\"messages\",\n",
    "                model=\"claude-3-5-haiku-20241022\",\n",
    "                temperature=0.7,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if providers_available[\"gemini\"]:\n",
    "        model_configs.append(\n",
    "            iModel(provider=\"gemini\", model=\"gemini-2.0-flash-exp\", temperature=0.7)\n",
    "        )\n",
    "\n",
    "    if not model_configs:\n",
    "        print(\"⚠️ No providers configured. Set at least one API key.\")\n",
    "        return\n",
    "\n",
    "    for model in model_configs:\n",
    "        session.services.register(model)\n",
    "\n",
    "    question = \"What is the difference between AI and machine learning?\"\n",
    "\n",
    "    print(f\"Question: {question}\\n\")\n",
    "\n",
    "    # Ask same question to all models\n",
    "    for model in model_configs:\n",
    "        branch = session.create_branch(name=f\"{model.name}-branch\")\n",
    "\n",
    "        result = await communicate(\n",
    "            session=session,\n",
    "            branch=branch,\n",
    "            parameters={\n",
    "                \"instruction\": question,\n",
    "                \"imodel\": model.name,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Provider: {model.backend.config.provider}\")\n",
    "        print(f\"Model: {model.backend.config.model}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(result)\n",
    "        print()\n",
    "\n",
    "\n",
    "await compare_providers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Dynamic Provider Switching\n",
    "\n",
    "Switch providers based on requirements (cost, speed, capabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def smart_chat(question: str, priority: str = \"balanced\"):\n",
    "    \"\"\"\n",
    "    Choose provider based on priority:\n",
    "    - 'speed': Groq (fastest)\n",
    "    - 'cost': Gemini (most affordable)\n",
    "    - 'quality': OpenAI GPT-4o (highest quality)\n",
    "    - 'balanced': Claude Haiku (good balance)\n",
    "    \"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Provider selection strategy\n",
    "    strategies = {\n",
    "        \"speed\": (\"groq\", \"llama-3.1-70b-versatile\", {}),\n",
    "        \"cost\": (\"gemini\", \"gemini-2.0-flash-exp\", {}),\n",
    "        \"quality\": (\"openai\", \"gpt-4o\", {}),\n",
    "        \"balanced\": (\"anthropic\", \"claude-3-5-haiku-20241022\", {\"endpoint\": \"messages\"}),\n",
    "    }\n",
    "\n",
    "    provider, model_name, kwargs = strategies.get(priority, strategies[\"balanced\"])\n",
    "\n",
    "    # Check if provider is available\n",
    "    if not providers_available.get(provider, False):\n",
    "        print(f\"⚠️ {provider} not configured. Trying fallback...\")\n",
    "        # Try first available provider\n",
    "        for p, available in providers_available.items():\n",
    "            if available:\n",
    "                provider = p\n",
    "                if provider == \"openai\":\n",
    "                    model_name = \"gpt-4o-mini\"\n",
    "                    kwargs = {}\n",
    "                elif provider == \"anthropic\":\n",
    "                    model_name = \"claude-3-5-haiku-20241022\"\n",
    "                    kwargs = {\"endpoint\": \"messages\"}\n",
    "                elif provider == \"gemini\":\n",
    "                    model_name = \"gemini-2.0-flash-exp\"\n",
    "                    kwargs = {}\n",
    "                elif provider == \"groq\":\n",
    "                    model_name = \"llama-3.1-70b-versatile\"\n",
    "                    kwargs = {}\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError(\"No providers configured\")\n",
    "\n",
    "    model = iModel(provider=provider, model=model_name, temperature=0.7, **kwargs)\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"smart-chat\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": question,\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Priority: {priority}\")\n",
    "    print(f\"Selected: {provider} / {model_name}\")\n",
    "    print(f\"\\nResponse:\\n{result}\\n\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Try different priorities\n",
    "question = \"Explain REST APIs in one sentence.\"\n",
    "\n",
    "for priority in [\"speed\", \"cost\", \"balanced\"]:\n",
    "    try:\n",
    "        await smart_chat(question, priority=priority)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with {priority}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Custom Endpoints\n",
    "\n",
    "Use custom API endpoints for providers like Groq, OpenRouter, or self-hosted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_groq():\n",
    "    \"\"\"Chat with Groq - extremely fast inference\"\"\"\n",
    "    if not providers_available.get(\"groq\", False):\n",
    "        print(\"⚠️ Groq not configured. Set GROQ_API_KEY to run this example.\")\n",
    "        return\n",
    "\n",
    "    session = Session()\n",
    "\n",
    "    # Groq configuration - blazing fast\n",
    "    model = iModel(\n",
    "        provider=\"groq\",\n",
    "        model=\"llama-3.1-70b-versatile\",  # or \"mixtral-8x7b-32768\"\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"groq-chat\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Explain quantum computing in simple terms\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Groq Response (fast!):\\n{result}\\n\")\n",
    "    return result\n",
    "\n",
    "\n",
    "await chat_groq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Base URL (Self-hosted or Proxy)\n",
    "\n",
    "You can point to custom API endpoints for self-hosted models or API proxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom OpenAI-compatible endpoint\n",
    "# (Don't run this unless you have a custom endpoint)\n",
    "\n",
    "\n",
    "async def chat_custom_endpoint():\n",
    "    \"\"\"Example with custom API endpoint (not runnable without actual endpoint)\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    model = iModel(\n",
    "        provider=\"openai\",  # OpenAI-compatible API\n",
    "        model=\"custom-model\",\n",
    "        api_key=\"your-custom-key\",\n",
    "        base_url=\"https://your-api-proxy.com/v1\",  # Custom endpoint\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    session.services.register(model)\n",
    "\n",
    "    branch = session.create_branch(name=\"custom-chat\")\n",
    "\n",
    "    result = await communicate(\n",
    "        session=session,\n",
    "        branch=branch,\n",
    "        parameters={\n",
    "            \"instruction\": \"Hello from custom endpoint!\",\n",
    "            \"imodel\": model.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Custom endpoint example (code only - requires actual endpoint)\")\n",
    "# await chat_custom_endpoint()  # Uncomment if you have a custom endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Provider Resilience with Fallback Chain\n",
    "\n",
    "Implement automatic fallback to handle provider outages or rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_with_fallback(question: str):\n",
    "    \"\"\"Try multiple providers until one succeeds\"\"\"\n",
    "    session = Session()\n",
    "\n",
    "    # Define fallback chain (order by preference)\n",
    "    providers = [\n",
    "        (\"openai\", \"gpt-4o-mini\", {}),\n",
    "        (\"anthropic\", \"claude-3-5-haiku-20241022\", {\"endpoint\": \"messages\"}),\n",
    "        (\"gemini\", \"gemini-2.0-flash-exp\", {}),\n",
    "        (\"groq\", \"llama-3.1-70b-versatile\", {}),\n",
    "    ]\n",
    "\n",
    "    for provider, model_name, kwargs in providers:\n",
    "        # Skip if provider not configured\n",
    "        if not providers_available.get(provider, False):\n",
    "            print(f\"⊘ {provider} not configured, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            model = iModel(provider=provider, model=model_name, temperature=0.7, **kwargs)\n",
    "            session.services.register(model)\n",
    "            branch = session.create_branch(name=f\"{provider}-branch\")\n",
    "\n",
    "            result = await communicate(\n",
    "                session=session,\n",
    "                branch=branch,\n",
    "                parameters={\n",
    "                    \"instruction\": question,\n",
    "                    \"imodel\": model.name,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(f\"✓ Success with {provider}\")\n",
    "            print(f\"\\nResponse:\\n{result}\\n\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {provider} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(\"All providers failed\")\n",
    "\n",
    "\n",
    "# Test fallback chain\n",
    "await chat_with_fallback(\"What is recursion?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider Comparison Reference\n",
    "\n",
    "| Provider | Endpoint | Best For | Speed | Cost |\n",
    "|----------|----------|----------|-------|------|\n",
    "| OpenAI | `chat/completions` (default) | General purpose, reasoning | Medium | Medium |\n",
    "| Anthropic | `messages` (required) | Long context, analysis | Medium | Medium |\n",
    "| Gemini | Default | Multimodal, free tier | Fast | Low |\n",
    "| Groq | Default | Speed-critical apps | Very Fast | Low |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Wrong endpoint for Anthropic** - Always use `endpoint=\"messages\"`\n",
    "2. **Missing API keys** - Set environment variables before running\n",
    "3. **Provider-specific model names** - Each provider has different model identifiers\n",
    "4. **Rate limits** - Implement retry logic or fallback chains for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Unified API**: Same code works with all providers\n",
    "2. **Easy Switching**: Change `provider` and `model` parameters only\n",
    "3. **Fallback Chains**: Build resilient systems with automatic provider failover\n",
    "4. **Flexible Configuration**: Support for custom endpoints and parameters\n",
    "5. **Provider-Specific Optimizations**: Each provider has unique strengths\n",
    "\n",
    "**Next Steps:**\n",
    "- Learn about [structured outputs](03_structured_outputs.ipynb)\n",
    "- Explore [multi-turn conversations](04_multi_turn_chat.ipynb)\n",
    "- Try [tool calling](05_tool_calling.ipynb)\n",
    "\n",
    "**Resources:**\n",
    "- [Chat Cookbook](../docs/cookbook/chat.md)\n",
    "- [API Reference: iModel](../docs/api/services.md)\n",
    "- [API Reference: Providers](../docs/api/providers.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
