{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 001: Claim Extraction with Operation Graphs\n",
    "\n",
    "Demonstrates sequential coordination using Operation Graphs to orchestrate ReaderTool workflows for academic claim extraction.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Sequential Coordination**: Building context step-by-step through dependent operations\n",
    "2. **ReaderTool Integration**: Document chunking and progressive reading strategies\n",
    "3. **Structured Extraction**: Using Pydantic models for reliable claim extraction\n",
    "4. **Custom Operations**: How to register and use custom workflow patterns\n",
    "\n",
    "## Use Case: Extracting Claims from Academic Papers\n",
    "\n",
    "We'll extract verifiable claims from an arxiv paper (Mamba: Linear-Time Sequence Modeling) by:\n",
    "- Opening the document with ReaderTool\n",
    "- Progressive analysis of document structure\n",
    "- Extracting specific claims with structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "# ReaderTool from lionagi\n",
    "from lionagi.tools.types import ReaderTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from lionpride import Builder, Session\n",
    "from lionpride.operations import (\n",
    "    GenerateParams,\n",
    ")\n",
    "from lionpride.services.types import iModel\n",
    "\n",
    "# Target document - Mamba paper from arxiv\n",
    "here = Path().cwd()\n",
    "document_path = here / \"data\" / \"mamba_paper.pdf\"\n",
    "\n",
    "print(\"Environment setup complete\")\n",
    "print(f\"Target: {document_path.name}\")\n",
    "print(\"Goal: Extract verifiable claims using coordinated operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data models for structured responses\n",
    "\n",
    "\n",
    "class Claim(BaseModel):\n",
    "    \"\"\"A single verifiable claim from the document.\"\"\"\n",
    "\n",
    "    claim: str = Field(..., description=\"The specific claim text\")\n",
    "    type: Literal[\"citation\", \"performance\", \"technical\", \"theoretical\", \"other\"] = Field(\n",
    "        ..., description=\"Category of claim\"\n",
    "    )\n",
    "    location: str = Field(..., description=\"Section/page reference\")\n",
    "    verifiability: Literal[\"high\", \"medium\", \"low\"] = Field(\n",
    "        ..., description=\"How easily this claim can be verified\"\n",
    "    )\n",
    "    search_strategy: str = Field(..., description=\"How to verify this claim\")\n",
    "\n",
    "\n",
    "class ClaimExtraction(BaseModel):\n",
    "    \"\"\"Collection of extracted claims.\"\"\"\n",
    "\n",
    "    claims: list[Claim] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class DocumentOutline(BaseModel):\n",
    "    \"\"\"Document structure analysis.\"\"\"\n",
    "\n",
    "    title: str = Field(..., description=\"Document title\")\n",
    "    sections: list[str] = Field(default_factory=list, description=\"Main sections\")\n",
    "    key_topics: list[str] = Field(default_factory=list, description=\"Key topics covered\")\n",
    "    claim_density_sections: list[str] = Field(\n",
    "        default_factory=list, description=\"Sections likely to contain verifiable claims\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Data models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session with model and ReaderTool\n",
    "\n",
    "# Create model\n",
    "model = iModel(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    name=\"gpt4o-mini\",\n",
    ")\n",
    "\n",
    "# Create session with default branch that has access to the model\n",
    "session = Session(\n",
    "    default_generate_model=model,\n",
    "    default_branch=\"main\",\n",
    ")\n",
    "\n",
    "# Initialize ReaderTool\n",
    "reader = ReaderTool()\n",
    "\n",
    "print(f\"Session: {session}\")\n",
    "print(f\"Default branch: {session.default_branch}\")\n",
    "print(f\"Operations: {session.operations.list_names()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Pattern 1: Direct Operations via Session.conduct()\n",
    "\n",
    "The simplest way to run operations - directly through the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Open document with ReaderTool\n",
    "doc_info = reader.handle_request({\"action\": \"open\", \"path_or_url\": str(document_path)})\n",
    "\n",
    "print(f\"Document opened: {doc_info.doc_info.doc_id}\")\n",
    "print(f\"Total length: {doc_info.doc_info.length:,} characters\")\n",
    "print(f\"Estimated tokens: {doc_info.doc_info.num_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read first chunk to understand structure\n",
    "first_chunk = reader.handle_request(\n",
    "    {\n",
    "        \"action\": \"read\",\n",
    "        \"doc_id\": doc_info.doc_info.doc_id,\n",
    "        \"start_offset\": 0,\n",
    "        \"end_offset\": 8000,  # ~2000 tokens\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Read {len(first_chunk.chunk.content):,} characters\")\n",
    "print(\"\\n--- First 500 chars ---\")\n",
    "print(first_chunk.chunk.content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use communicate operation to analyze structure\n",
    "from lionpride.types import Operable, Spec\n",
    "\n",
    "# Create operable for DocumentOutline\n",
    "outline_operable = Operable(\n",
    "    specs=(\n",
    "        Spec(name=\"title\", base_type=str),\n",
    "        Spec(name=\"sections\", base_type=str, listable=True),\n",
    "        Spec(name=\"key_topics\", base_type=str, listable=True),\n",
    "        Spec(name=\"claim_density_sections\", base_type=str, listable=True),\n",
    "    ),\n",
    "    name=\"DocumentOutline\",\n",
    ")\n",
    "\n",
    "# Run communicate operation\n",
    "op = await session.conduct(\n",
    "    \"communicate\",\n",
    "    generate=GenerateParams(\n",
    "        imodel=\"gpt4o-mini\",\n",
    "        instruction=\"Analyze this document excerpt and identify its structure.\",\n",
    "        context={\"document_content\": first_chunk.chunk.content},\n",
    "    ),\n",
    "    operable=outline_operable,\n",
    "    capabilities={\"title\", \"sections\", \"key_topics\", \"claim_density_sections\"},\n",
    ")\n",
    "\n",
    "print(f\"Operation status: {op.status}\")\n",
    "print(\"\\nDocument outline:\")\n",
    "print(op.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Pattern 2: Operation Graphs with Builder\n",
    "\n",
    "For multi-step workflows with dependencies, use Builder to construct operation graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def claim_extraction_workflow():\n",
    "    \"\"\"Sequential workflow: analyze structure -> read key sections -> extract claims.\"\"\"\n",
    "\n",
    "    builder = Builder()\n",
    "\n",
    "    # Step 1: Analyze document structure\n",
    "    builder.add(\n",
    "        name=\"analyze_structure\",\n",
    "        operation=\"communicate\",\n",
    "        parameters={\n",
    "            \"generate\": {\n",
    "                \"imodel\": \"gpt4o-mini\",\n",
    "                \"instruction\": (\n",
    "                    \"Analyze this academic paper excerpt. Identify the main sections \"\n",
    "                    \"and which sections are most likely to contain verifiable claims \"\n",
    "                    \"(performance benchmarks, citations, technical specifications).\"\n",
    "                ),\n",
    "                \"context\": {\"document_content\": first_chunk.chunk.content},\n",
    "            },\n",
    "            \"return_as\": \"text\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Step 2: Extract claims (depends on structure analysis)\n",
    "    builder.add(\n",
    "        name=\"extract_claims\",\n",
    "        operation=\"communicate\",\n",
    "        parameters={\n",
    "            \"generate\": {\n",
    "                \"imodel\": \"gpt4o-mini\",\n",
    "                \"instruction\": (\n",
    "                    \"Based on the document structure analysis, extract 5-7 specific, \"\n",
    "                    \"verifiable claims from this paper. Focus on:\\n\"\n",
    "                    \"- Performance benchmarks and metrics\\n\"\n",
    "                    \"- Technical specifications\\n\"\n",
    "                    \"- Citations that can be verified\\n\"\n",
    "                    \"- Theoretical claims with mathematical basis\"\n",
    "                ),\n",
    "                \"context\": {\"document_content\": first_chunk.chunk.content},\n",
    "            },\n",
    "            \"return_as\": \"text\",\n",
    "        },\n",
    "        depends_on=[\"analyze_structure\"],\n",
    "        inherit_context=True,\n",
    "    )\n",
    "\n",
    "    # Build and execute graph\n",
    "    graph = builder.build()\n",
    "    print(f\"Graph: {len(graph.nodes)} operations, {len(graph.edges)} dependencies\")\n",
    "\n",
    "    results = await session.flow(graph, verbose=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Execute workflow\n",
    "results = await claim_extraction_workflow()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WORKFLOW RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(result[:1000] if isinstance(result, str) else result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Pattern 3: Custom Operations for Nested DAGs\n",
    "\n",
    "Register custom operations that can contain their own nested workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Define a custom operation that contains a nested DAG\n\n\nasync def deep_claim_analysis(session, branch, params):\n    \"\"\"Custom operation with nested DAG for deep claim analysis.\n\n    This demonstrates arbitrary nested workflow patterns:\n    1. Outer operation receives parameters\n    2. Builds inner DAG based on context\n    3. Executes inner flow\n    4. Aggregates results\n    \"\"\"\n    from lionpride import Builder\n\n    document_content = params.get(\"document_content\", \"\")\n    # num_passes can be used to control iteration depth in more complex workflows\n    _ = params.get(\"num_passes\", 2)\n\n    # Build inner DAG with multiple analysis passes\n    inner_builder = Builder()\n\n    # First pass: broad analysis\n    inner_builder.add(\n        name=\"broad_scan\",\n        operation=\"communicate\",\n        parameters={\n            \"generate\": {\n                \"imodel\": \"gpt4o-mini\",\n                \"instruction\": \"Identify all potential claims in this text. Be exhaustive.\",\n                \"context\": {\"text\": document_content[:4000]},\n            },\n            \"return_as\": \"text\",\n        },\n    )\n\n    # Second pass: validation (depends on first)\n    inner_builder.add(\n        name=\"validate_claims\",\n        operation=\"communicate\",\n        parameters={\n            \"generate\": {\n                \"imodel\": \"gpt4o-mini\",\n                \"instruction\": (\n                    \"Review these potential claims. For each, assess:\\n\"\n                    \"1. Is it actually a claim (not just description)?\\n\"\n                    \"2. Is it verifiable?\\n\"\n                    \"3. What evidence would validate/refute it?\"\n                ),\n            },\n            \"return_as\": \"text\",\n        },\n        depends_on=[\"broad_scan\"],\n        inherit_context=True,\n    )\n\n    # Third pass: synthesis (depends on validation)\n    inner_builder.add(\n        name=\"synthesize\",\n        operation=\"communicate\",\n        parameters={\n            \"generate\": {\n                \"imodel\": \"gpt4o-mini\",\n                \"instruction\": (\n                    \"Synthesize the validated claims into a final list. \"\n                    \"Include only high-confidence, verifiable claims.\"\n                ),\n            },\n            \"return_as\": \"text\",\n        },\n        depends_on=[\"validate_claims\"],\n        inherit_context=True,\n    )\n\n    # Execute inner DAG\n    inner_graph = inner_builder.build()\n    inner_results = await session.flow(inner_graph, branch, verbose=True)\n\n    # Return aggregated results\n    return {\n        \"passes\": list(inner_results.keys()),\n        \"final_claims\": inner_results.get(\"synthesize\", \"\"),\n        \"validation_notes\": inner_results.get(\"validate_claims\", \"\"),\n    }\n\n\n# Register custom operation\nsession.register_operation(\"deep_claim_analysis\", deep_claim_analysis)\nprint(f\"Registered operations: {session.operations.list_names()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the custom operation\n",
    "op = await session.conduct(\n",
    "    \"deep_claim_analysis\",\n",
    "    document_content=first_chunk.chunk.content,\n",
    "    num_passes=3,\n",
    ")\n",
    "\n",
    "print(f\"\\nCustom operation status: {op.status}\")\n",
    "print(f\"\\nPasses executed: {op.response.get('passes', [])}\")\n",
    "print(\"\\n--- Final Claims ---\")\n",
    "print(op.response.get(\"final_claims\", \"\")[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Pattern 4: Nested DAG in Builder Graph\n",
    "\n",
    "Custom operations can be used inside Builder graphs, enabling arbitrary nesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def nested_dag_workflow():\n",
    "    \"\"\"Outer graph that uses custom operations containing inner graphs.\"\"\"\n",
    "\n",
    "    builder = Builder()\n",
    "\n",
    "    # Step 1: Standard operation\n",
    "    builder.add(\n",
    "        name=\"prepare_context\",\n",
    "        operation=\"communicate\",\n",
    "        parameters={\n",
    "            \"generate\": {\n",
    "                \"imodel\": \"gpt4o-mini\",\n",
    "                \"instruction\": \"Summarize the key themes in this document for claim extraction.\",\n",
    "                \"context\": {\"document_content\": first_chunk.chunk.content[:2000]},\n",
    "            },\n",
    "            \"return_as\": \"text\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Step 2: Custom operation with nested DAG\n",
    "    builder.add(\n",
    "        name=\"deep_analysis\",\n",
    "        operation=\"deep_claim_analysis\",  # Our custom operation!\n",
    "        parameters={\n",
    "            \"document_content\": first_chunk.chunk.content,\n",
    "            \"num_passes\": 3,\n",
    "        },\n",
    "        depends_on=[\"prepare_context\"],\n",
    "        inherit_context=True,\n",
    "    )\n",
    "\n",
    "    # Step 3: Final aggregation\n",
    "    builder.add(\n",
    "        name=\"final_report\",\n",
    "        operation=\"communicate\",\n",
    "        parameters={\n",
    "            \"generate\": {\n",
    "                \"imodel\": \"gpt4o-mini\",\n",
    "                \"instruction\": (\n",
    "                    \"Create a final report summarizing the extracted claims. \"\n",
    "                    \"Format as a structured list with verification strategies.\"\n",
    "                ),\n",
    "            },\n",
    "            \"return_as\": \"text\",\n",
    "        },\n",
    "        depends_on=[\"deep_analysis\"],\n",
    "        inherit_context=True,\n",
    "    )\n",
    "\n",
    "    # Execute\n",
    "    graph = builder.build()\n",
    "    print(f\"Outer graph: {len(graph.nodes)} operations\")\n",
    "    print(\"Note: 'deep_analysis' contains its own inner DAG with 3 operations!\")\n",
    "\n",
    "    results = await session.flow(graph, verbose=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Execute nested workflow\n",
    "nested_results = await nested_dag_workflow()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NESTED DAG RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOperations completed: {list(nested_results.keys())}\")\n",
    "print(\"\\n--- Final Report ---\")\n",
    "print(nested_results.get(\"final_report\", \"\")[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This cookbook demonstrated:\n",
    "\n",
    "1. **Direct Operations**: `session.conduct()` for simple workflows\n",
    "2. **Operation Graphs**: `Builder` + `session.flow()` for multi-step DAGs\n",
    "3. **Custom Operations**: `session.register_operation()` for arbitrary patterns\n",
    "4. **Nested DAGs**: Custom operations containing inner graphs\n",
    "\n",
    "The key insight is that lionpride's operation system is **compositional**:\n",
    "- Operations are just functions `(session, branch, params) -> result`\n",
    "- Custom operations can call `session.flow()` with inner graphs\n",
    "- This enables arbitrary nesting depth for complex workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
